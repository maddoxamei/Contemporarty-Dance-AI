{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import utils\n",
    "#No module named keras OR cannot import name 'np_utils' if tensorflow.keras\n",
    "#from keras.utils import np_utils\n",
    "#from keras.models import load_model\n",
    "#from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-perameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters are based off of the 3 layer model in \n",
    "    Recurrent Neural Networks for Modeling Motion Capture Data \n",
    "    by Mir Khan, Heikki Huttunen, Olli Suominen and Atanas Gotchev\n",
    "\"\"\"\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001) # Maintain a moving (discounted) average of the square of gradients\n",
    "# The folowing initializers are applied to all hidden layers of the model before training \n",
    "weight_initializer = keras.initializers.Orthogonal(gain=1.0, seed=None) # Generates an orthogonal matrix with multiplicative factor equal to the gain\n",
    "recurrent_initializer = tf.keras.initializers.GlorotNormal(seed=None) # Draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (n_input_weight_units + n_output_weight_units))\n",
    "bias_initializer = keras.initializers.Zeros() # Set biases to zero\n",
    "layer_activation = 'tanh'\n",
    "recurrent_activation = 'hard_sigmoid'\n",
    "output_activation = 'linear'\n",
    "\n",
    "batch_size = 32 #number of samples trained before performing one step of gradient update for the loss function (default is stochastic gradient descent)\n",
    "look_back = 30 #how many frames will be used as a history for prediction\n",
    "offset = 1 #how many frames in the future is the prediction going to occur at\n",
    "forecast = 1 #how many frames will be predicted\n",
    "sample_increment = 7 #number of frames between each sample\n",
    "epochs = 30 #maximum number of times all training samples are fed into the model for training\n",
    "units = 843 #number of nodes in each hidden layer of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 165 #number of columns in the input dimension\n",
    "frames = 500 #number of frames the model should generate\n",
    "training_split = 0.7 #the proportion of the data to use for training\n",
    "validation_split = 0.2 #the proportion of the data to use for validating during the training phase at the end of each epoch\n",
    "evaluation_split = 0.1 #(test_split) the proportion of the data for evaluating the model effectiveness after training completion\n",
    "\n",
    "csv_data_dir = \"/Akamai/MLDance/data/CSV/Raw\" #directory to the csv representation of the dances\n",
    "np_data_dir = \"/Akamai/MLDance/data/Numpy\" #directory to the numpy representation of the dances\n",
    "logs_dir = \"/Akamai/MLDance/logs\" #general output directory\n",
    "\n",
    "data_identifier = \"lb-{}_o-{}_f-{}-si-{}\".format(look_back, offset,forecast, sample_increment) #data-specific string for use in creating readily identifiable filenames\n",
    "model_identifier = \"units-{}_timesteps-{}\".format(units, look_back) #model-specific string for use in creating readily identifiable filenames\n",
    "split_identifier = \"ts-{}_vs-{}_es-{}\".format(training_split, validation_split, evaluation_split) #split-specific string for use in creating readily identifiable filenames\n",
    "save_dir = os.path.join(logs_dir, model_identifier) #output directory for model-specific content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(os.path.join(save_dir, \"outfile.txt\"), \"w\")\n",
    "#utils.write(\"test\", out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below block in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "        self.predict = False\n",
    "        self.evaluate = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do NOT run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mmaddox/.local/share/jupyter/runtime/kernel-5ea024af-4d96-463e-bec5-226f641204de.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#store_true: default is False, sets the value to True if the respective tag is called\n",
    "#store_false: default is True, sets the value to False if the respective tag is called\n",
    "parser.add_argument('--train', action=\"store_true\",\n",
    "                   help='Train on dataset')\n",
    "parser.add_argument('--evaluate', action=\"store_true\",\n",
    "                   help='Run an evaluation on the trained model')\n",
    "parser.add_argument('--predict', action=\"store_true\",\n",
    "                   help='Generate a dance using the trained model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    \"\"\" Create the cooresponding directory files for the given path if it does not yet exist\n",
    "\n",
    "    :param path: proposed directory filepath\n",
    "    :type str\n",
    "    :return: created directory filepath\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    utils.create_dir(path)\n",
    "\n",
    "def get_unique_dance_names():\n",
    "    \"\"\" Aggregate the names of unique dances from the CSV data directory\n",
    "    \n",
    "    :return: the dance names where there are csv files for\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return utils.get_unique_dance_names(csv_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Related\n",
    "\n",
    "**Save/Load Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(filename):\n",
    "    \"\"\" Fetch the pre-procced data cooresponding to the given dance name\n",
    "\n",
    "    :param filename: the name of the dance to grab the data from\n",
    "    :type str\n",
    "    :return: the pre-processed dance data\n",
    "    :rtype: numpy.Array\n",
    "    \"\"\"\n",
    "    csv_filename = os.path.join(csv_data_dir, filename)\n",
    "    np_filename = os.path.join(np_data_dir, filename+\"_\"+data_identifier+\"_ts-{}\".format(training_split))\n",
    "    return utils.get_processed_data(csv_filename, np_filename, training_split)\n",
    "    \n",
    "def save_generated_dance(generated_data, original_filename, save_filename):\n",
    "    \"\"\" Save the generated dance to a csv file for bvh converstion later.\n",
    "\n",
    "    :param generated_data: the name of the dance to grab the data from\n",
    "    :type numpy.Array\n",
    "    :param original_filename: dance name the generation seed is from\n",
    "    :type str\n",
    "    :param save_filename: the directory and filename to store the generated dance at\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    hierarchy_file = os.path.join(csv_data_dir, \"hierarchy/\"+original_filename.split('_')[0]+\"_hierarchy.csv\")\n",
    "    original_data = pd.read_csv(os.path.join(csv_data_dir, original_filename+\"_rotations.csv\"), nrows=0)\n",
    "    c_headers = [c for c in original_data.columns if 'End' not in c ][1:]\n",
    "    utils.save_generated_dance(generated_data, training_split, hierarchy_file, c_headers, save_filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence the Data (Separate Into Samples) Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    \"\"\" Fetch the pre-sequenced or sampled data from the given dance, or create/save it if it does not yet exist\n",
    "\n",
    "    :param filename: the name of the dance to sample data from\n",
    "    :type str\n",
    "    :return: the collection of input X and target Y samples for the train, validation, and evaluation datasets\n",
    "    :type tuple\n",
    "    \"\"\"    \n",
    "    csv_filename = os.path.join(csv_data_dir, filename)\n",
    "    np_filename = os.path.join(np_data_dir, filename+\"_\"+data_identifier+\"_ts-{}\".format(training_split))\n",
    "    return utils.get_sample_data(csv_filename, np_filename, look_back, offset, forecast, sample_increment, training_split, validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_model(feature_size):\n",
    "    \"\"\" Establish the architecture (layers and how they are connected*) of the model with freshly initialized state for the weights. \n",
    "        There is NO compilation information.\n",
    "\n",
    "    :param feature_size: the number of features in the input/output vector\n",
    "    :type int\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.establish_model(units, look_back, feature_size, layer_activation, recurrent_activation, weight_initializer, recurrent_initializer, bias_initializer, output_activation)\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\" Compile the given model so that it is ready for training and/or prediction/evaluation\n",
    "\n",
    "    :param model: the model to compile\n",
    "    :type keras.Model\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.compile_model(model, optimizer, 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and Load Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_architecture(model, identifier):\n",
    "    \"\"\" Save the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights and no compilation information from this savefile\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    json_config = model.to_jason()\n",
    "    print(json_donfig)\n",
    "    \n",
    "def save_weights(model, logs=None):\n",
    "    \"\"\" Save the model weights. Ideal for use during training to create checkpoints.\n",
    "        Weights can be loaded into a model (ideally the original checkpointed model) to extract the desired weights/layers into the saved mode\n",
    "\n",
    "    :param model: the model to save the weights from\n",
    "    :type keras.Model\n",
    "    :param logs: dictionary containing current model specs\n",
    "    :type dict\n",
    "    \"\"\"\n",
    "    save_file = \"weights_{}_\".format(look_back)+model_identifier+\"_loss-{:.2f}_acc-{:.2f}.h5\".format(logs[\"loss\"], logs[\"accuracy\"])\n",
    "    utils.save_weights(model, save_dir, save_file)\n",
    "    \n",
    "def save_trained_model(model, identifier):\n",
    "    \"\"\" Save the entire model. Model can be loaded and restart training right where you left off\n",
    "        The following are saved:\n",
    "            weight values\n",
    "            Model's architecture\n",
    "            Model's training configuration (what you pass to the .compile() method)\n",
    "            Optimizer and its state, if any (this allows you to restart training)\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    utils.save_trained_model(model, save_dir, identifier)\n",
    "    \n",
    "def load_architecture(file):\n",
    "    \"\"\" Load the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights.\n",
    "        There is NO compilation information in this savefile.\n",
    "\n",
    "    :param file: .json file which holds the model's architecture data\n",
    "    :type str\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.load_architecture(file)\n",
    "\n",
    "def load_trained_model(file):\n",
    "    \"\"\" Load the pre-trained model. Compiled when loaded so training/prediction/evaluation can be restarted right where the model left off. \n",
    "\n",
    "    :param file: .h5 file which holds the model's information\n",
    "    :type str\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.load_trained_model(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \"\"\" A class to create custom callback options. This overrides a set of methods called at various stages of training, testing, and predicting. \n",
    "        Callbacks are useful to get a view on internal states and statistics of the model during training.\n",
    "            Callback list can be passed for .fit(), .evaluate(), and .predict() methods\n",
    "            \n",
    "        keys = list(logs.keys())\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.start_time = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        print(\"\\tTraining, Beginning:\", self.start_time)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Training Complete \", \"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        print(\"\\tEvaluation, Beginning:\", self.start_time)\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"Evaluation Complete \", \"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        print(\"\\tPredicting, Beginning:\", self.start_time)\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        print(\"Predicting Complete \", \"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\" Trains the model with the dance data.\n",
    "        The History object's History.history attribute is a record of training loss values and metrics values at successive epochs, \n",
    "            as well as cooresponding validation values (if applicable).  \n",
    "\n",
    "    :param model: the model to train\n",
    "    :type keras.Model\n",
    "    :return: the class containing the training metric information and the trained model\n",
    "    :type tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    dances = get_unique_dance_names()\n",
    "    checkpoint_filename = \"weights_{}_\".format(look_back)+model_identifier+\"_epoch-{epoch:02d}_loss-{loss:.2f}_acc-{accuracy:.2f}_val-loss-{val_loss:.2f}_val-acc-{val_accuracy:.2f}.h5\"\n",
    "    checkpoint_filepath = os.path.join(save_dir, checkpoint_filename)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    callbacks_list = [checkpoint, early_stopping, CustomCallback()]\n",
    "\n",
    "    comprehensive_train_X = np.array([])\n",
    "    comprehensive_train_Y = np.array([])\n",
    "    comprehensive_validate_X = np.array([])\n",
    "    comprehensive_validate_Y = np.array([])\n",
    "    comprehensive_evaluation_X = np.array([])\n",
    "    comprehensive_evaluation_Y = np.array([])\n",
    "    \n",
    "    print(\"Fetching and Agregating Training Data ...\")\n",
    "    for dance in utils.progressbar(dances, \"Progress: \"):\n",
    "        train_X, train_Y, validate_X, validate_Y, evaluation_X, evaluation_Y = get_sample_data(dance)\n",
    "        if(len(comprehensive_train_X)==0):\n",
    "            comprehensive_train_X = train_X\n",
    "            comprehensive_train_Y = train_Y\n",
    "            comprehensive_validate_X = validate_X\n",
    "            comprehensive_validate_Y = validate_Y\n",
    "            comprehensive_evaluation_X = evaluation_X\n",
    "            comprehensive_evaluation_Y = evaluation_Y\n",
    "        else:\n",
    "            comprehensive_train_X = np.vstack((comprehensive_train_X,train_X))\n",
    "            comprehensive_train_Y = np.vstack((comprehensive_train_Y,train_Y))\n",
    "            comprehensive_validate_X = np.vstack((comprehensive_validate_X,validate_X))\n",
    "            comprehensive_validate_Y = np.vstack((comprehensive_validate_Y,validate_Y))\n",
    "            comprehensive_evaluation_X = np.vstack((comprehensive_evaluation_X,evaluation_X))\n",
    "            comprehensive_evaluation_Y = np.vstack((comprehensive_evaluation_Y,evaluation_Y))\n",
    "      \n",
    "    start_time = time.time()\n",
    "    history = model.fit(comprehensive_train_X, comprehensive_train_Y, \n",
    "                        batch_size = batch_size, \n",
    "                        callbacks=callbacks_list, \n",
    "                        validation_data= (comprehensive_validate_X, comprehensive_validate_Y),\n",
    "                        epochs=epochs, \n",
    "                        verbose=1)\n",
    "    \n",
    "    save_trained_model(model, model_identifier)\n",
    "    evaluation_save = os.path.join(np_data_dir, \"comprehensive_evaluation_\"+data_identifier+\"_es-{}\".format(evaluation_split))\n",
    "    np.save(evaluation_save+\"_X\", comprehensive_evaluation_X)\n",
    "    np.save(evaluation_save+\"_Y\", comprehensive_evaluation_Y)\n",
    "    with open(os.path.join(save_dir, \"history_train_\"+model_identifier+\"_\"+data_identifier+\"_\"+split_identifier+\".json\"), \"w\") as history_file:  \n",
    "        json.dump(pd.DataFrame.from_dict(history.history).to_dict(), history_file) \n",
    "    print(\"Saved metric history to json file\")\n",
    "    return history, model, comprehensive_evaluation_X, comprehensive_evaluation_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample/Run Model (Make Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, n_frames, random_frame=False):\n",
    "    \"\"\" Generate a dance sequence with the given model\n",
    "\n",
    "    :param model: the model to use for prediction \n",
    "    :type keras.Model\n",
    "    :param n_frames: the number of frames the model should generate\n",
    "    :type int\n",
    "    \"\"\"\n",
    "    #select random dance for seed\n",
    "    dances = get_unique_dance_names()\n",
    "    seed_dance_index = random.randint(0, len(dances) - 1)\n",
    "    dance = get_processed_data(dances[seed_dance_index])\n",
    "    seed = dance[:look_back]\n",
    "    if random_frame:\n",
    "        #select random frame(s) for seed\n",
    "        seed_frame_index = random.randint(0, len(dance) - (look_back+1))\n",
    "        seed = dance[seed_frame_index:seed_frame_index+look_back]\n",
    "    \n",
    "    print(\"Generating dance with seed from\", dances[seed_dance_index])\n",
    "    #for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    for diversity in [1.0]:\n",
    "        start_time = time.time()\n",
    "        generated = seed\n",
    "        for i in utils.progressbar(range(n_frames),\"{} Progress: \".format(diversity)):\n",
    "            preds = model.predict(np.array([generated[-look_back:]]), verbose=0)[0]\n",
    "            generated = np.vstack((generated, preds))\n",
    "        filename = os.path.join(save_dir, \"generated_dance_{}-frames_{}-diversity\".format(n_frames, diversity))\n",
    "        save_generated_dance(generated, dances[seed_dance_index], filename)\n",
    "        \n",
    "        print(\"\\tSaved to\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a88900128dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a88900128dc6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mAny\u001b[0m \u001b[0mexpansion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhowever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mable\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredefined\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwhatever\u001b[0m \u001b[0mpurpose\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msave_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Driver function to control what is run and when if this is the main python script being ran.\n",
    "        As the project was developed in a jupyter notebook, everything is self-contained in the main file.\n",
    "        Any expansion, however, would be able to use the predefined classes and functions for whatever purpose without running anything.\n",
    "    \"\"\"\n",
    "    reload(utils)\n",
    "    save_location = utils.create_dir(save_dir)\n",
    "    history, model, eval_X, eval_Y = None, None, None, None\n",
    "    if(not args.train and not args.evaluate and not args.predict):\n",
    "        print(\"Type -h and get a list of possible tasks. You may select multiple.\")\n",
    "    else:\n",
    "        if(args.train):\n",
    "            model = establish_model(n_features)\n",
    "            model = compile_model(model)\n",
    "            history, model, eval_X, eval_Y = train_model(model)\n",
    "        if(args.evaluate):\n",
    "            if(not model):\n",
    "                #loads the most recent saved model\n",
    "                filename = [f for f in os.listdir(save_dir) if \"model\" in f][-1]\n",
    "                model = load_trained_model(os.path.join(save_location, filename))\n",
    "                print(model.summary())\n",
    "                eval_X = np.load(os.path.join(np_data_dir, \"comprehensive_evaluation_\"+data_identifier+\"_es-{}\".format(evaluation_split)+\"_X.npy\"))\n",
    "                eval_Y = np.load(os.path.join(np_data_dir, \"comprehensive_evaluation_\"+data_identifier+\"_es-{}\".format(evaluation_split)+\"_Y.npy\"))\n",
    "            history = model.evaluate(eval_X, eval_Y, callbacks=[], verbose=1)\n",
    "            with open(os.path.join(save_dir, \"history_eval_\"+model_identifier+\"_\"+data_identifier+\"_\"+split_identifier+\".json\"), \"w\") as history_file:  \n",
    "                json.dump(pd.DataFrame.from_dict(history.history).to_dict(), history_file) \n",
    "        print(\"Saved metric history to json file\")\n",
    "        if(args.predict):\n",
    "            if(not model):\n",
    "                #loads the most recent saved model\n",
    "                filename = [f for f in os.listdir(save_dir) if \"model\" in f][-1]\n",
    "                model = load_trained_model(os.path.join(save_location, filename))\n",
    "                print(model.summary())\n",
    "            benchmark(model, frames)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
