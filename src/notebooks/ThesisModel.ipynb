{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#No module named keras OR cannot import name 'np_utils' if tensorflow.keras\n",
    "#from keras.utils import np_utils\n",
    "#from keras.models import load_model\n",
    "#from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Whether Train or Sample** \n",
    "\n",
    "Run below block in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "        self.stateful = True\n",
    "        self.n_timesteps = 20\n",
    "        self.n_epochs = 50\n",
    "        self.n_units = 256\n",
    "        self.n_frames = 500\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do NOT run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-40-23cb9e155ac8>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-23cb9e155ac8>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    help='How many dance frames should be used as history during prediction')\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#store_true sets the default to False\n",
    "#store_false sets the default to True\n",
    "parser.add_argument('-train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "parser.add_argument('-stateful', action=\"store_false\",\n",
    "                   help='True: Remeber previous state during training, False: Feed forward model')\n",
    "parser.add_argument('-n_timesteps', type=int, const=1, default=args.n_timesteps\n",
    "                   help='How many dance frames should be used as history during prediction')\n",
    "parser.add_argument('-n_epochs', type=int, const=1, default=args.n_epochs\n",
    "                   help='Number of repititions the data is fed into the model')\n",
    "parser.add_argument('-n_units', type=int, const=1, default=args.n_units\n",
    "                   help='The number of dance frames the AI should generate')\n",
    "\n",
    "parser.add_argument('-n_frames', type=int, const=1, default=args.n_frames\n",
    "                   help='The number of dance frames the AI should generate')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dances = []\n",
    "BATCH_SIZE = 1\n",
    "STEPS = 1\n",
    "N_TIMESTEPS = args.n_timesteps # Sequence length\n",
    "N_EPOCHS = args.n_epochs\n",
    "N_NODES = args.n_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local run only\n",
    "csv_data_dir = \"../../../data/CSV/Raw\"\n",
    "np_data_dir = \"../../../data/Numpy\"\n",
    "save_dir = \"../../../logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For server run\n",
    "csv_data_dir = \"/Akamai/MLDance/data/CSV/Raw\"\n",
    "np_data_dir = \"/Akamai/MLDance/data/Numpy\"\n",
    "save_dir = \"/Akamai/MLDance/logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, file=sys.stdout):\n",
    "    count = len(it)\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        file.write(\"%s[%s%s] %i/%i\\r\" % (prefix, \"#\"*x, \".\"*(size-x), j, count))\n",
    "        file.flush()        \n",
    "    show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    file.write(\"\\n\")\n",
    "    file.flush()\n",
    "\n",
    "def getFileNames():\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames): #enumerating creates an array where 0 corresponds to the index of the file in filenames and 1 corresponds to the filename\n",
    "        filenames[file[0]] = '_'.join(file[1].split(\"_\")[:-1])\n",
    "    return list(set(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Pre-Process Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    filename = os.path.join(csv_data_dir, filename)\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"_worldpos.csv\")\n",
    "    rot_data = pd.read_csv(filename+\"_rotations.csv\")\n",
    "    data = rot_data.copy()\n",
    "\n",
    "    #standardize rotation (force values from -1 to 1)\n",
    "    data = data/180.0\n",
    "\n",
    "    #Add the root (hip) position data for spacial movement to the rotational data\n",
    "    data['Hips.Pos.X'] = pos_data.pop('Hips.X')\n",
    "    data['Hips.Pos.Y'] = pos_data.pop('Hips.Y')\n",
    "    data['Hips.Pos.Z'] = pos_data.pop('Hips.Z')\n",
    "\n",
    "    #Normalize the starting positions of the given dance\n",
    "    #Making movement relative to an origin of 0,0,0 for consistancy within different dances\n",
    "    data['Hips.Pos.X'] = data['Hips.Pos.X'] + (-1*data['Hips.Pos.X'][0])\n",
    "    data['Hips.Pos.Y'] = data['Hips.Pos.Y'] + (-1*data['Hips.Pos.Y'][0])\n",
    "    data['Hips.Pos.Z'] = data['Hips.Pos.Z'] + (-1*data['Hips.Pos.Z'][0])\n",
    "    \n",
    "    #Remove the all the columns were it's all zeroed (End ones)\n",
    "    zeroed_columns = [column for column in data.columns if 'End' in column]\n",
    "    for column in zeroed_columns:\n",
    "        data.pop(column)\n",
    "\n",
    "    #remove the time variable from the dataset\n",
    "    time = data.pop('Time') #maybe change to time change value instead? To indicate speed\n",
    "    return data\n",
    "\n",
    "def get_processed_data(filename):\n",
    "    #Establish filenames (X is for input, Y is for expected output)\n",
    "    loaded = os.path.join(np_data_dir, filename)\n",
    "    \n",
    "    #If the corresponding numpy file doesn't yet exist, create and save it\n",
    "    if not (os.path.exists(loaded+\".npy\")):\n",
    "        #Print statement for status update\n",
    "        print(\"Creating pre-processed datafile:\", filename)\n",
    "        #load the csv file and establish the number of rows and columns\n",
    "        data = pre_process_data(filename)\n",
    "        data = data.iloc[:].values #Enables selection/edit of cells in the dataset\n",
    "        print(\"Saving pre-processed data\")\n",
    "        np.save(loaded, data)\n",
    "\n",
    "    return np.load(loaded+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence the Data (Separate Into Samples)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_frames predict the next frame\n",
    "def sequence_individual_frame(data, dataX, dataY, i):\n",
    "    seqIn = data[i: i+N_TIMESTEPS]\n",
    "    seqOut = data[i+N_TIMESTEPS]\n",
    "    dataX.append(seqIn)\n",
    "    dataY.append(seqOut)\n",
    "        \n",
    "#n_frames predict the next n_frames\n",
    "def sequence_multiple_frames(data, dataX, dataY, i):\n",
    "    seqIn = data[i: i+N_TIMESTEPS]\n",
    "    seqOut = data[i+N_TIMESTEPS : i+N_TIMESTEPS+1]\n",
    "    dataX.append(seqIn)\n",
    "    dataY.append(seqOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequenced_data(filename):\n",
    "    #Establish filenames (X is for input, Y is for expected output)\n",
    "    loadedX = os.path.join(np_data_dir, filename+\"X-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, filename+\"Y-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    #If the corresponding numpy file doesn't yet exist, create and save it\n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        #Print statement for status update\n",
    "        print(\"Creating the sequenced data\")\n",
    "        #load the csv file and establish the number of rows and columns\n",
    "        data = get_processed_data(filename)\n",
    "        N_ROWS = data.values.shape[0]\n",
    "        N_COLOMNS = data.values.shape[1]\n",
    "\n",
    "        #data = data.iloc[:].values #Enables selection/edit of cells in the dataset\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        \n",
    "        #Generate the sequences\n",
    "        for i in range(0, N_ROWS - N_TIMESTEPS, STEPS): #range(start, stop, step) \n",
    "            sequence_multiple_frames(data, dataX, dataY, i)\n",
    "\n",
    "        #X shape [samples, timesteps, features]\n",
    "        #Y shape [samples, 1, features]\n",
    "        X, Y = np.array(dataX), np.array(dataY)\n",
    "\n",
    "        N_SAMPLES = len(dataX)\n",
    "        Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "        print(\"Saving the sequenced data\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_stateful_model(N_COLOMNS):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='tanh', \n",
    "                                batch_input_shape = (BATCH_SIZE, N_TIMESTEPS, N_COLOMNS),\n",
    "                                return_sequences=True, \n",
    "                                stateful=True))\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='tanh', return_sequences=True, stateful=True))\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='tanh', return_sequences=False, stateful=True))\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh')) #Setting it stateful is the same as TimeDistributedDense(Dense()) --> (#, None, #)\n",
    "    return model\n",
    "\n",
    "def establish_non_stateful_model(N_COLOMNS):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='tanh', \n",
    "                                input_shape = (N_TIMESTEPS, N_COLOMNS), \n",
    "                                #batch_size = BATCH_SIZE, \n",
    "                                return_sequences=True))\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='tanh'))\n",
    "    #model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    if args.stateful:\n",
    "        model = establish_stateful_model(165)\n",
    "    else:\n",
    "        model = establish_non_stateful_model(165)\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy']) #\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch):\n",
    "    temp_path = os.path.join(save_dir, \"units-{}_timesteps-{}\".format(N_NODES, N_TIMESTEPS))\n",
    "    if not os.path.exists(temp_path):\n",
    "        os.makedirs(temp_path)\n",
    "    savefile_weights = \"weights-\"+str(N_TIMESTEPS)+\"_loss-{:.4f}_acc-{:.4f}.h5\".format(logs[\"loss\"], logs[\"accuracy\"])\n",
    "    savefile_model = \"model_epoch-{}.h5\".format(epoch)\n",
    "    #json_string = model.to_jason() #architecture\n",
    "    model.save_weights(os.path.join(temp_path, savefile_weights)) #weights\n",
    "    if epoch%3==0:\n",
    "        model.save(os.path.join(temp_path, savefile_model))\n",
    "            \n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None): #changed from (epoch, _) was None\n",
    "        save_model(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = create_model()\n",
    "    \n",
    "    dances = getFileNames()\n",
    "    \n",
    "    for i in range(N_EPOCHS):\n",
    "        #callbacks_list = [MyCustomCallback()]\n",
    "        callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
    "        \n",
    "        for dance in dances:\n",
    "            print(\"Epoch \",str(i)+\"/\"+str(N_EPOCHS)+\":\", dance)\n",
    "            X, Y = get_sample_data(dance)\n",
    "            \n",
    "            print (\"X Shape:\", X.shape)\n",
    "            print (\"Y Shape:\", Y.shape)\n",
    "            #train/fit the model\n",
    "            model.fit(X, Y, \n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      callbacks=callbacks_list, \n",
    "                      shuffle=False, \n",
    "                      validation_split=0.2, \n",
    "                      epochs=1, \n",
    "                      verbose=1)\n",
    "        save_model(i)\n",
    "        random.shuffle(dances)\n",
    "    \n",
    "    print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    return keras.models.load_model(os.path.join(save_dir, filename), compile=True)\n",
    "\n",
    "def temperature(preds, temperature=1.0): #Not yet altered\n",
    "    print(preds)\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    print (preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def save_to_csv(generated_data, original_file, save_file):    \n",
    "    rotation = generated_data[:,:162] #get the first 162 columns\n",
    "    position = generated_data[:,[162, 163, 164]] #get the last 3 columns\n",
    "    hierarchy = pd.read_csv(os.path.join(csv_data_dir, \"hierarchy/\"+original_file.split('_')[0]+\"_hierarchy.csv\"))\n",
    "    \n",
    "    data = pd.read_csv(os.path.join(csv_data_dir, original_file+\"_rotations.csv\"), nrows=0)\n",
    "    c_headers = [c for c in data.columns if 'End' not in c ][1:]\n",
    "    rotation_df = pd.DataFrame(rotation, columns=c_headers)\n",
    "    position_df = pd.DataFrame(position, columns=c_headers[:3])\n",
    "\n",
    "    #undo the normalization and standardization of the data\n",
    "    rotation_df = rotation_df*180\n",
    "    position_df['Hips.X'] = position_df['Hips.X'] + hierarchy['offset.x'][0]\n",
    "    position_df['Hips.Y'] = position_df['Hips.Y'] + hierarchy['offset.y'][0]\n",
    "    position_df['Hips.Z'] = position_df['Hips.Z'] + hierarchy['offset.z'][0]\n",
    "    \n",
    "    new_headers = []\n",
    "    joints = [j for j in hierarchy['joint'].to_numpy() if \"End\" not in j]\n",
    "    for j in joints:\n",
    "        new_headers.append(j+\".Z\")\n",
    "        new_headers.append(j+\".X\")\n",
    "        new_headers.append(j+\".Y\")\n",
    "\n",
    "    rotation_df = rotation_df.reindex(columns=new_headers)  \n",
    "    \n",
    "    rotation_df.insert(0, 'time', np.arange(0.0, 0.03333333*len(rotation_df), 0.03333333))\n",
    "    position_df.insert(0, 'time', np.arange(0.0, 0.03333333*len(position_df), 0.03333333))\n",
    "    \n",
    "    rotation_df.to_csv(save_file+\"_rot.csv\", index=False)\n",
    "    position_df.to_csv(save_file+\"_pos.csv\", index=False)\n",
    "    \n",
    "def benchmark(n_frames, units=N_NODES, timesteps=N_TIMESTEPS):\n",
    "    #loads the most recent saved model\n",
    "    temp_path = os.path.join(save_dir, \"units-{}_timesteps-{}\".format(units, timesteps))\n",
    "    filename = [f for f in os.listdir(temp_path) if \"model\" in f][-1]\n",
    "    model = load_model(os.path.join(temp_path, filename))\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    #select random dance for seed\n",
    "    dances = getFileNames()\n",
    "    seed_index = random.randint(0, len(dances) - 1)\n",
    "    seed = get_processed_data(dances[seed_index])[:N_TIMESTEPS]\n",
    "    \n",
    "    print(\"Generating dance with seed from\", dances[seed_index])\n",
    "    #for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    for diversity in [1.0]:\n",
    "        start_time = time.time()\n",
    "        generated = seed\n",
    "        for i in progressbar(range(n_frames),\"{} Progress: \".format(diversity)):\n",
    "            preds = model.predict(np.array([generated[-N_TIMESTEPS:]]), verbose=0)[0]\n",
    "            generated = np.vstack((generated, preds))\n",
    "        save_location = os.path.join(temp_path, \"generated_dance_{}-frames_{}-diversity\".format(n_frames, diversity))\n",
    "        save_to_csv(generated, dances[seed_index], save_location)\n",
    "        print(\"\\tTotal Elapsed time (in sec.):\", time.time()-start_time)\n",
    "        print(\"\\tSaved to\", save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Creating pre-processed datafile: Andria_Bored_v1_0\n",
      "Saving pre-processed data\n",
      "Generating dance with seed from Andria_Bored_v1_0\n",
      "1.0 Progress: [############################################################] 500/500\n",
      "\tTotal Elapsed time (in sec.): 27.179787635803223\n",
      "Saved the generated dance to /Akamai/MLDance/logs/units-256_timesteps-20/generated_dance_500-frames_1.0-diversity\n"
     ]
    }
   ],
   "source": [
    "if(args.train):\n",
    "    start_time = time.time()\n",
    "    train_model()\n",
    "    print(\"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "else:\n",
    "    benchmark(args.n_frames, args.n_units, args.n_timesteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
