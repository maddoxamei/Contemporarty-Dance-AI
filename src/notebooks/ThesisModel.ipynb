{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#No module named keras OR cannot import name 'np_utils' if tensorflow.keras\n",
    "#from keras.utils import np_utils\n",
    "#from keras.models import load_model\n",
    "#from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Whether Train or Sample** \n",
    "\n",
    "Run below block in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.stateful = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do NOT run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "parser.add_argument('-stateful', action=\"store_true\",\n",
    "                   help='True: Remeber previous state during training, False: Feed forward model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local run only\n",
    "csv_data_dir = \"../../../data/CSV/Raw\"\n",
    "np_data_dir = \"../../../data/Numpy\"\n",
    "save_dir = \"../../../logs\"\n",
    "dances = []\n",
    "BATCH_SIZE = 20\n",
    "STEPS = 1\n",
    "N_TIMESTEPS = 20 # Sequence length\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For server run\n",
    "csv_data_dir = \"/Akamai/MLDance/data/CSV/Raw\"\n",
    "np_data_dir = \"/Akamai/MLDance/data/Numpy\"\n",
    "save_dir = \"/Akamai/MLDance/logs\"\n",
    "dances = []\n",
    "BATCH_SIZE = 1\n",
    "N_TIMESTEPS = 20 # Sequence length\n",
    "N_EPOCHS = 15\n",
    "N_NODES = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull Names of Dance Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames): #enumerating creates an array where 0 corresponds to the index of the file in filenames and 1 corresponds to the filename\n",
    "        filenames[file[0]] = '_'.join(file[1].split(\"_\")[:-1])\n",
    "    return set(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Process Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    filename = os.path.join(csv_data_dir, filename)\n",
    "    print(filename)\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"_worldpos.csv\")\n",
    "    rot_data = pd.read_csv(filename+\"_rotations.csv\")\n",
    "    data = rot_data.copy()\n",
    "\n",
    "    #standardize rotation (force values from -1 to 1)\n",
    "    data = data/180.0\n",
    "\n",
    "    #Add the root (hip) position data for spacial movement to the rotational data\n",
    "    data['Hips.Pos.X'] = pos_data.pop('Hips.X')\n",
    "    data['Hips.Pos.Y'] = pos_data.pop('Hips.Y')\n",
    "    data['Hips.Pos.Z'] = pos_data.pop('Hips.Z')\n",
    "\n",
    "    #Normalize the starting positions of the given dance\n",
    "    #Making movement relative to an origin of 0,0,0 for consistancy within different dances\n",
    "    data['Hips.Pos.X'] = data['Hips.Pos.X'] + (-1*data['Hips.Pos.X'][0])\n",
    "    data['Hips.Pos.Y'] = data['Hips.Pos.Y'] + (-1*data['Hips.Pos.Y'][0])\n",
    "    data['Hips.Pos.Z'] = data['Hips.Pos.Z'] + (-1*data['Hips.Pos.Z'][0])\n",
    "    \n",
    "    #Remove the all the columns were it's all zeroed (End ones)\n",
    "    zeroed_columns = [column for column in data.columns if 'End' in column]\n",
    "    for column in zeroed_columns:\n",
    "        data.pop(column)\n",
    "\n",
    "    #remove the time variable from the dataset\n",
    "    time = data.pop('Time') #maybe change to time change value instead? To indicate speed\n",
    "    data.head()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data and Separate Into Samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_by_frame(dataX, dataY, i):\n",
    "    seqIn = data[i: i+N_TIMESTEPS]\n",
    "    seqOut = data[i+N_TIMESTEPS]\n",
    "    dataX.append(seqIn)\n",
    "    dataY.append(seqOut)\n",
    "        \n",
    "def sequence_by_frames(dataX, dataY, i):\n",
    "    seqIn = data[i: i+N_TIMESTEPS]\n",
    "    seqOut = data[i+N_TIMESTEPS : i+N_TIMESTEPS+1]\n",
    "    dataX.append(seqIn)\n",
    "    dataY.append(seqOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    #Establish filenames (X is for input, Y is for expected output)\n",
    "    loadedX = os.path.join(np_data_dir, filename+\"X-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, filename+\"Y-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    #If the corresponding numpy file doesn't yet exist, create and save it\n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        #Print statement for status update\n",
    "        print(\"create\")\n",
    "        #load the csv file and establish the number of rows and columns\n",
    "        data = pre_process_data(os.path.join(csv_data_dir, filename))\n",
    "        N_ROWS = data.values.shape[0]\n",
    "        N_COLOMNS = data.values.shape[1]\n",
    "\n",
    "        data = data.iloc[:].values #Enables selection/edit of cells in the dataset\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        \n",
    "        #Generate the sequences\n",
    "        for i in range(0, N_ROWS - N_TIMESTEPS, STEPS): #range(start, stop, step) \n",
    "            sequence_by_frames(dataX, dataY, i)\n",
    "\n",
    "        #X shape [samples, timesteps, features]\n",
    "        #Y shape [samples, 1, features]\n",
    "        X, Y = np.array(dataX), np.array(dataY)\n",
    "\n",
    "        N_SAMPLES = len(dataX)\n",
    "        Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "        print(\"saving\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_stateful_model(N_COLOMNS):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='relu', \n",
    "                                batch_input_shape = (BATCH_SIZE, N_TIMESTEPS, N_COLOMNS),\n",
    "                                return_sequences=True, \n",
    "                                stateful=True))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='relu', stateful=True))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def establish_non_stateful_model(N_COLOMNS):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='relu', \n",
    "                                input_shape = (N_TIMESTEPS, N_COLOMNS), \n",
    "                                #batch_size = BATCH_SIZE, \n",
    "                                return_sequences=True))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(N_NODES, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    if args.stateful:\n",
    "        model = establish_stateful_model(165)\n",
    "    else:\n",
    "        model = establish_non_stateful_model(165)\n",
    "    model.compile(optimizer='adam', loss='mse') #metrics=['accuracy']\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}): #changed from (epoch, _) was None\n",
    "        savefile_weights = \"weights-\"+str(N_TIMESTEPS)+\"_{loss:.4f}.h5\"\n",
    "        savefile_model = \"model-\"+str(N_TIMESTEPS)+\"_{}.h5\".format(epoch)\n",
    "        #json_string = model.to_jason() #architecture\n",
    "        self.model.save_weights(os.path.join(save_dir, savefile_weights)) #weights\n",
    "        if epoch%2==0:\n",
    "            self.model.save(os.path.join(save_dir, savefile_model))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = create_model()\n",
    "    \n",
    "    dances = list(getFileNames())\n",
    "    \n",
    "    for i in range(N_EPOCHS):\n",
    "        print(str(i)+\"/\"+str(N_EPOCHS))\n",
    "        callbacks_list = [MyCustomCallback()]\n",
    "        \n",
    "        for dance in dances:\n",
    "            print(str(i)+\"/\"+str(N_EPOCHS)+\": on dance\", dance)\n",
    "            X, Y = get_sample_data(dance)\n",
    "            \n",
    "            print (\"X Shape:\", X.shape)\n",
    "            print (\"Y Shape:\", Y.shape)\n",
    "            #train/fit the model\n",
    "            model.fit(X, Y, batch_size = BATCH_SIZE, callbacks=callbacks_list)\n",
    "        random.shuffle(dances)\n",
    "    \n",
    "    print(\"Done Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 20, 256)           432128    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 165)               42405     \n",
      "=================================================================\n",
      "Total params: 999,845\n",
      "Trainable params: 999,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "0/15\n",
      "0/15: on dance Theodora_Satisfied_1\n",
      "X Shape: (730, 20, 165)\n",
      "Y Shape: (730, 165)\n",
      "37/37 [==============================] - 7s 82ms/step - loss: 49.6062\n",
      "\n",
      "Epoch 00001: loss improved from inf to 49.29102, saving model to ../../../logs\\Weights-Improvement-0-49.2910.h5\n",
      "\n",
      "Epoch 00001: loss improved from inf to 49.29102, saving model to ../../../logs\\Weights-Best.h5\n",
      "0/15: on dance Elena_Neutral_v2_2\n",
      "X Shape: (696, 20, 165)\n",
      "Y Shape: (696, 165)\n",
      "35/35 [==============================] - 2s 70ms/step - loss: 106.6988\n",
      "\n",
      "Epoch 00001: loss did not improve from 49.29102\n",
      "\n",
      "Epoch 00001: loss did not improve from 49.29102\n",
      "0/15: on dance Vasso_Afraid_v1_0\n",
      "X Shape: (567, 20, 165)\n",
      "Y Shape: (567, 165)\n",
      "29/29 [==============================] - 2s 60ms/step - loss: 86.9167\n",
      "\n",
      "Epoch 00001: loss did not improve from 49.29102\n",
      "\n",
      "Epoch 00001: loss did not improve from 49.29102\n",
      "0/15: on dance Andria_Sad_v1_0\n",
      "X Shape: (1048, 20, 165)\n",
      "Y Shape: (1048, 165)\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 34.8860\n",
      "\n",
      "Epoch 00001: loss improved from 49.29102 to 34.88601, saving model to ../../../logs\\Weights-Improvement-0-34.8860.h5\n",
      "\n",
      "Epoch 00001: loss improved from 49.29102 to 34.88601, saving model to ../../../logs\\Weights-Best.h5\n",
      "0/15: on dance Vasso_Miserable_0\n",
      "X Shape: (730, 20, 165)\n",
      "Y Shape: (730, 165)\n",
      "37/37 [==============================] - 2s 61ms/step - loss: 14.7906\n",
      "\n",
      "Epoch 00001: loss improved from 34.88601 to 14.79057, saving model to ../../../logs\\Weights-Improvement-0-14.7906.h5\n",
      "\n",
      "Epoch 00001: loss improved from 34.88601 to 14.79057, saving model to ../../../logs\\Weights-Best.h5\n",
      "0/15: on dance Elena_Relaxed_v1_1\n",
      "X Shape: (619, 20, 165)\n",
      "Y Shape: (619, 165)\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 64.5447\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Olivia_Excited_1\n",
      "X Shape: (730, 20, 165)\n",
      "Y Shape: (730, 165)\n",
      "37/37 [==============================] - 2s 63ms/step - loss: 50.4510\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Tired_v2_0\n",
      "X Shape: (900, 20, 165)\n",
      "Y Shape: (900, 165)\n",
      "45/45 [==============================] - 4s 60ms/step - loss: 50.7420\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Angry_v1_0\n",
      "X Shape: (828, 20, 165)\n",
      "Y Shape: (828, 165)\n",
      "42/42 [==============================] - 3s 60ms/step - loss: 30.8764\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Elena_Annoyed_v1_0\n",
      "X Shape: (632, 20, 165)\n",
      "Y Shape: (632, 165)\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 66.9286\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Olivia_Annoyed_0\n",
      "X Shape: (730, 20, 165)\n",
      "Y Shape: (730, 165)\n",
      "37/37 [==============================] - 2s 58ms/step - loss: 23.6223\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Elena_Excited_v2_1\n",
      "X Shape: (638, 20, 165)\n",
      "Y Shape: (638, 165)\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 83.3595\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Afraid_v1_0\n",
      "X Shape: (865, 20, 165)\n",
      "Y Shape: (865, 165)\n",
      "44/44 [==============================] - 3s 61ms/step - loss: 35.2791\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Sophie_Relaxed_1\n",
      "X Shape: (580, 20, 165)\n",
      "Y Shape: (580, 165)\n",
      "29/29 [==============================] - 2s 60ms/step - loss: 68.0938\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Vasso_Neutral_v3_2\n",
      "X Shape: (619, 20, 165)\n",
      "Y Shape: (619, 165)\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 28.1493\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Pleased_v2_1\n",
      "X Shape: (796, 20, 165)\n",
      "Y Shape: (796, 165)\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 55.3948\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Theodora_Happy_1\n",
      "X Shape: (730, 20, 165)\n",
      "Y Shape: (730, 165)\n",
      "37/37 [==============================] - 3s 79ms/step - loss: 79.0856\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Sophie_Sad_0\n",
      "X Shape: (580, 20, 165)\n",
      "Y Shape: (580, 165)\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 141.9403\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Sophie_Tired_0\n",
      "X Shape: (580, 20, 165)\n",
      "Y Shape: (580, 165)\n",
      "29/29 [==============================] - 2s 79ms/step - loss: 87.5327\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Satisfied_v2_1\n",
      "X Shape: (578, 20, 165)\n",
      "Y Shape: (578, 165)\n",
      "29/29 [==============================] - 2s 78ms/step - loss: 58.7100\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "\n",
      "Epoch 00001: loss did not improve from 14.79057\n",
      "0/15: on dance Andria_Happy_v1_1\n",
      "X Shape: (786, 20, 165)\n",
      "Y Shape: (786, 165)\n",
      "26/40 [==================>...........] - ETA: 1s - loss: 40.3580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-5e65a6430926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s hours ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-eed1076242fc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Y Shape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m#train/fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if(args.train):\n",
    "    start_time = time.time()\n",
    "    train_model()\n",
    "    print(\"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "else:\n",
    "    print(\"Will Sample in the Future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
