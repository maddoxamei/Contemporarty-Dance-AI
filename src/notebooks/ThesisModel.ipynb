{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import utils\n",
    "#No module named keras OR cannot import name 'np_utils' if tensorflow.keras\n",
    "#from keras.utils import np_utils\n",
    "#from keras.models import load_model\n",
    "#from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-perameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters are based off of the 3 layer model in \n",
    "    Recurrent Neural Networks for Modeling Motion Capture Data \n",
    "    by Mir Khan, Heikki Huttunen, Olli Suominen and Atanas Gotchev\n",
    "\"\"\"\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001) # Maintain a moving (discounted) average of the square of gradients\n",
    "# The folowing initializers are applied to all hidden layers of the model before training \n",
    "weight_initializer = keras.initializers.Orthogonal(gain=1.0, seed=None) # Generates an orthogonal matrix with multiplicative factor equal to the gain\n",
    "recurrent_initializer = tf.keras.initializers.GlorotNormal(seed=None) # Draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (n_input_weight_units + n_output_weight_units))\n",
    "bias_initializer = keras.initializers.Zeros() # Set biases to zero\n",
    "layer_activation = 'tanh'\n",
    "recurrent_activation = 'hard_sigmoid'\n",
    "output_activation = 'linear'\n",
    "\n",
    "batch_size = 32 #number of samples trained before performing one step of gradient update for the loss function (default is stochastic gradient descent)\n",
    "look_back = 50 #how many frames will be used as a history for prediction\n",
    "offset = 1 #how many frames in the future is the prediction going to occur at\n",
    "forecast = 1 #how many frames will be predicted\n",
    "sample_increment = 12 #number of frames between each sample\n",
    "epochs = 30 #maximum number of times all training samples are fed into the model for training\n",
    "units = 843 #number of nodes in each hidden layer of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 165 #number of columns in the input dimension\n",
    "frames = 500 #number of frames the model should generate\n",
    "training_split = 0.7 #the proportion of the data to use for training\n",
    "validation_split = 0.2 #the proportion of the data to use for validating during the training phase at the end of each epoch\n",
    "evaluation_split = 0.1 #(test_split) the proportion of the data for evaluating the model effectiveness after training completion\n",
    "\n",
    "csv_data_dir = \"/Akamai/MLDance/data/CSV/Raw\" #directory to the csv representation of the dances\n",
    "np_data_dir = \"/Akamai/MLDance/data/Numpy\" #directory to the numpy representation of the dances\n",
    "logs_dir = \"/Akamai/MLDance/logs\" #general output directory\n",
    "\n",
    "data_identifier = \"lb-{}_o-{}_f-{}-si-{}\".format(look_back, offset,forecast, sample_increment) #data-specific string for use in creating readily identifiable filenames\n",
    "model_identifier = \"units-{}_timesteps-{}\".format(units, look_back) #model-specific string for use in creating readily identifiable filenames\n",
    "save_dir = os.path.join(logs_dir, model_identifier) #output directory for model-specific content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below block in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do NOT run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mmaddox/.local/share/jupyter/runtime/kernel-59054712-6e28-48ef-bbd4-8006d335df7e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#store_true: default is False, sets the value to True if the respective tag is called\n",
    "#store_false: default is True, sets the value to False if the respective tag is called\n",
    "parser.add_argument('--train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    \"\"\" Create the cooresponding directory files for the given path if it does not yet exist\n",
    "\n",
    "    :param path: proposed directory filepath\n",
    "    :type str\n",
    "    :return: created directory filepath\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    utils.create_dir(path)\n",
    "\n",
    "def get_unique_dance_names():\n",
    "    \"\"\" Aggregate the names of unique dances from the CSV data directory\n",
    "    \n",
    "    :return: the dance names where there are csv files for\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    return utils.get_unique_dance_names(csv_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Related\n",
    "\n",
    "**Save/Load Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(filename):\n",
    "    \"\"\" Fetch the pre-procced data cooresponding to the given dance name\n",
    "\n",
    "    :param filename: the name of the dance to grab the data from\n",
    "    :type str\n",
    "    :return: the pre-processed dance data\n",
    "    :rtype: numpy.Array\n",
    "    \"\"\"\n",
    "    csv_filename = os.path.join(csv_data_dir, filename)\n",
    "    np_filename = os.path.join(np_data_dir, filename+\"_\"+data_identifier+\"_ts-{}\".format(training_split))\n",
    "    return utils.get_processed_data(csv_filename, np_filename, training_split)\n",
    "    \n",
    "def save_generated_dance(generated_data, original_filename, save_filename):\n",
    "    \"\"\" Save the generated dance to a csv file for bvh converstion later.\n",
    "\n",
    "    :param generated_data: the name of the dance to grab the data from\n",
    "    :type numpy.Array\n",
    "    :param original_filename: dance name the generation seed is from\n",
    "    :type str\n",
    "    :param save_filename: the directory and filename to store the generated dance at\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    hierarchy_file = os.path.join(csv_data_dir, \"hierarchy/\"+original_filename.split('_')[0]+\"_hierarchy.csv\")\n",
    "    original_data = pd.read_csv(os.path.join(csv_data_dir, original_filename+\"_rotations.csv\"), nrows=0)\n",
    "    c_headers = [c for c in original_data.columns if 'End' not in c ][1:]\n",
    "    utils.save_generated_dance(generated_data, training_split, hierarchy_file, c_headers, save_filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence the Data (Separate Into Samples) Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    \"\"\" Fetch the pre-sequenced or sampled data from the given dance, or create/save it if it does not yet exist\n",
    "\n",
    "    :param filename: the name of the dance to sample data from\n",
    "    :type str\n",
    "    :return: the collection of input X and target Y samples for the train, validation, and evaluation datasets\n",
    "    :type tuple\n",
    "    \"\"\"    \n",
    "    csv_filename = os.path.join(csv_data_dir, filename)\n",
    "    np_filename = os.path.join(np_data_dir, filename+\"_\"+data_identifier+\"_ts-{}\".format(training_split))\n",
    "    return utils.get_sample_data(csv_filename, np_filename, look_back, offset, forecast, sample_increment, training_split, validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_model(feature_size):\n",
    "    \"\"\" Establish the architecture (layers and how they are connected*) of the model with freshly initialized state for the weights. \n",
    "        There is NO compilation information.\n",
    "\n",
    "    :param feature_size: the number of features in the input/output vector\n",
    "    :type int\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.establish_model(units, look_back, feature_size, layer_activation, recurrent_activation, weight_initializer, recurrent_initializer, bias_initializer, output_activation)\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\" Compile the given model so that it is ready for training and/or prediction/evaluation\n",
    "\n",
    "    :param model: the model to compile\n",
    "    :type keras.Model\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.compile_model(model, optimizer, 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and Load Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_architecture(model, identifier):\n",
    "    \"\"\" Save the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights and no compilation information from this savefile\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    json_config = model.to_jason()\n",
    "    print(json_donfig)\n",
    "    \n",
    "def save_weights(model, logs=None):\n",
    "    \"\"\" Save the model weights. Ideal for use during training to create checkpoints.\n",
    "        Weights can be loaded into a model (ideally the original checkpointed model) to extract the desired weights/layers into the saved mode\n",
    "\n",
    "    :param model: the model to save the weights from\n",
    "    :type keras.Model\n",
    "    :param logs: dictionary containing current model specs\n",
    "    :type dict\n",
    "    \"\"\"\n",
    "    save_file = \"weights_{}_\".format(look_back)+model_identifier+\"_loss-{:.2f}_acc-{:.2f}.h5\".format(logs[\"loss\"], logs[\"accuracy\"])\n",
    "    utils.save_weights(model, save_dir, save_file)\n",
    "    \n",
    "def save_trained_model(model, identifier):\n",
    "    \"\"\" Save the entire model. Model can be loaded and restart training right where you left off\n",
    "        The following are saved:\n",
    "            weight values\n",
    "            Model's architecture\n",
    "            Model's training configuration (what you pass to the .compile() method)\n",
    "            Optimizer and its state, if any (this allows you to restart training)\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    utils.save_trained_model(model, save_dir, identifier)\n",
    "    \n",
    "def load_architecture(file):\n",
    "    \"\"\" Load the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights.\n",
    "        There is NO compilation information in this savefile.\n",
    "\n",
    "    :param file: .json file which holds the model's architecture data\n",
    "    :type str\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.load_architecture(file)\n",
    "\n",
    "def load_trained_model(file):\n",
    "    \"\"\" Load the pre-trained model. Compiled when loaded so training/prediction/evaluation can be restarted right where the model left off. \n",
    "\n",
    "    :param file: .h5 file which holds the model's information\n",
    "    :type str\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return utils.load_trained_model(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \"\"\" A class to create custom callback options. This overrides a set of methods called at various stages of training, testing, and predicting. \n",
    "        Callbacks are useful to get a view on internal states and statistics of the model during training.\n",
    "            Callback list can be passed for .fit(), .evaluate(), and .predict() methods\n",
    "            \n",
    "        keys = list(logs.keys())\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        save_trained_model(model, model_identifier+\"_epoch-{0:0=2d}\".format(epoch))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        save_weights(model, logs)\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\" Trains the model with the dance data.\n",
    "        The History object's History.history attribute is a record of training loss values and metrics values at successive epochs, \n",
    "            as well as cooresponding validation values (if applicable).  \n",
    "\n",
    "    :param model: the model to train\n",
    "    :type keras.Model\n",
    "    :return: the class containing the training metric information and the trained model\n",
    "    :type tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    dances = get_unique_dance_names()\n",
    "    callbacks_list = [CustomCallback(), tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)]\n",
    "    \n",
    "    comprehensive_train_X = np.array([])\n",
    "    comprehensive_train_Y = np.array([])\n",
    "    comprehensive_validate_X = np.array([])\n",
    "    comprehensive_validate_Y = np.array([])\n",
    "    comprehensive_evaluation_X = np.array([])\n",
    "    comprehensive_evaluation_Y = np.array([])\n",
    "    \n",
    "    print(\"Fetching and Agregating Training Data ...\")\n",
    "    for dance in dances:\n",
    "        train_X, train_Y, validate_X, validate_Y, evaluation_X, evaluation_Y = get_sample_data(dance)\n",
    "        if(len(comprehensive_train_X)==0):\n",
    "            comprehensive_train_X = train_X\n",
    "            comprehensive_train_Y = train_Y\n",
    "            comprehensive_validate_X = validate_X\n",
    "            comprehensive_validate_Y = validate_Y\n",
    "            comprehensive_evaluation_X = evaluation_X\n",
    "            comprehensive_evaluation_Y = evaluation_Y\n",
    "        else:\n",
    "            comprehensive_train_X = np.vstack((comprehensive_train_X,train_X))\n",
    "            comprehensive_train_Y = np.vstack((comprehensive_train_Y,train_Y))\n",
    "            comprehensive_validate_X = np.vstack((comprehensive_validate_X,validate_X))\n",
    "            comprehensive_validate_Y = np.vstack((comprehensive_validate_Y,validate_Y))\n",
    "            comprehensive_evaluation_X = np.vstack((comprehensive_evaluation_X,evaluation_X))\n",
    "            comprehensive_evaluation_Y = np.vstack((comprehensive_evaluation_Y,evaluation_Y))\n",
    "      \n",
    "    start_time = time.time()\n",
    "    history = model.fit(comprehensive_train_X, comprehensive_train_Y, \n",
    "                        batch_size = batch_size, \n",
    "                        callbacks=callbacks_list, \n",
    "                        validation_data= (comprehensive_validate_X, comprehensive_validate_Y),\n",
    "                        epochs=epochs, \n",
    "                        verbose=1)\n",
    "    \n",
    "    print(\"Training Complete \", \"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "    return history, model, comprehensive_evaluation_X, comprehensive_evaluation_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample/Run Model (Make Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, n_frames, random_frame=False):\n",
    "    \"\"\" Generate a dance sequence with the given model\n",
    "\n",
    "    :param model: the model to use for prediction \n",
    "    :type keras.Model\n",
    "    :param n_frames: the number of frames the model should generate\n",
    "    :type int\n",
    "    \"\"\"\n",
    "    #select random dance for seed\n",
    "    dances = get_unique_dance_names()\n",
    "    seed_dance_index = random.randint(0, len(dances) - 1)\n",
    "    dance = get_processed_data(dances[seed_dance_index])\n",
    "    seed = dance[:look_back]\n",
    "    if random_frame:\n",
    "        #select random frame(s) for seed\n",
    "        seed_frame_index = random.randint(0, len(dance) - (look_back+1))\n",
    "        seed = dance[seed_frame_index:seed_frame_index+look_back]\n",
    "    \n",
    "    print(\"Generating dance with seed from\", dances[seed_dance_index])\n",
    "    #for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    for diversity in [1.0]:\n",
    "        start_time = time.time()\n",
    "        generated = seed\n",
    "        for i in utils.progressbar(range(n_frames),\"{} Progress: \".format(diversity)):\n",
    "            preds = model.predict(np.array([generated[-look_back:]]), verbose=0)[0]\n",
    "            generated = np.vstack((generated, preds))\n",
    "        filename = os.path.join(save_dir, \"generated_dance_{}-frames_{}-diversity\".format(n_frames, diversity))\n",
    "        save_generated_dance(generated, dances[seed_dance_index], filename)\n",
    "        print(\"\\tTotal Elapsed time (in sec.):\", time.time()-start_time)\n",
    "        print(\"\\tSaved to\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_111 (LSTM)              (None, 50, 1024)          4874240   \n",
      "_________________________________________________________________\n",
      "lstm_112 (LSTM)              (None, 50, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_113 (LSTM)              (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 165)               169125    \n",
      "=================================================================\n",
      "Total params: 21,828,773\n",
      "Trainable params: 21,828,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fetching and Agregating Training Data ...\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v4_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v4_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v4_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v4_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Bored_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v2_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Excited_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Sad_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Tired_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Mix_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Relaxed_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Scary_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Scary_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Scary_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Scary_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Pleased_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Pleased_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Pleased_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Happy_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Afraid_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Miserable_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Annoyed_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Bored_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Happy_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Satisfied_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v3_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Neutral_v3_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v3_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Neutral_v3_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Nervous_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Nervous_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Nervous_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Nervous_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Annoyed_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Neutral_v1_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Neutral_v1_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Neutral_v1_2_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Neutral_v1_2_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Angry_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Bored_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Annoyed_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Angry_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Satisfied_v2_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Afraid_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Active_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Active_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Active_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Active_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Happy_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Sad_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Miserable_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Tired_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Theodora_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Theodora_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Tired_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Excited_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Relaxed_v1_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Sad_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Excited_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Olivia_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Olivia_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Satisfied_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Sophie_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Sophie_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Sophie_Miserable_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Andria_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Andria_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Andria_Angry_v2_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Elena_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Elena_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Elena_Afraid_v1_0_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Vasso_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Vasso_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Vasso_Relaxed_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating the sequenced data: /Akamai/MLDance/data/Numpy/Anna_Curiosity_1_lb-50_o-1_f-1_ts-0.7\n",
      "Creating pre-processed datafile: /Akamai/MLDance/data/Numpy/Anna_Curiosity_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Curiosity_1_lb-50_o-1_f-1_ts-0.7\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Anna_Curiosity_1_lb-50_o-1_f-1_ts-0.7\n",
      "Train on 2989 samples, validate on 705 samples\n",
      "Epoch 1/30\n",
      "2989/2989 [==============================] - 175s 58ms/sample - loss: 12.8444 - accuracy: 0.9886 - val_loss: 6.6139 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "2989/2989 [==============================] - 167s 56ms/sample - loss: 2.1764 - accuracy: 0.9997 - val_loss: 1.4981 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "2989/2989 [==============================] - 168s 56ms/sample - loss: 1.5421 - accuracy: 0.9993 - val_loss: 1.6269 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "2989/2989 [==============================] - 169s 57ms/sample - loss: 1.2997 - accuracy: 0.9997 - val_loss: 1.2720 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "2989/2989 [==============================] - 167s 56ms/sample - loss: 1.1213 - accuracy: 0.9997 - val_loss: 1.0817 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "2989/2989 [==============================] - 167s 56ms/sample - loss: 0.9830 - accuracy: 0.9997 - val_loss: 0.9134 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "2989/2989 [==============================] - 166s 56ms/sample - loss: 0.8947 - accuracy: 0.9997 - val_loss: 0.8834 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      " 544/2989 [====>.........................] - ETA: 2:17 - loss: 0.7998 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-dbfe0dc099fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-dbfe0dc099fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestablish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#loads the most recent saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-709f397c36cc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomprehensive_validate_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomprehensive_validate_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         verbose=1)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Complete \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--- %s hours ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             batch_end=step * batch_size + current_batch_size)\n\u001b[1;32m    173\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_exhausted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         self.callbacks._call_batch_hook(\n\u001b[0;32m--> 700\u001b[0;31m             mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-95c1961bbde7>\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-90be07eddc97>\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(model, logs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weights_{}_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_identifier\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_loss-{:.2f}_acc-{:.2f}.h5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Driver function to control what is run and when if this is the main python script being ran.\n",
    "        As the project was developed in a jupyter notebook, everything is self-contained in the main file.\n",
    "        Any expansion, however, would be able to use the predefined classes and functions for whatever purpose without running anything.\n",
    "    \"\"\"\n",
    "    reload(utils)\n",
    "    save_location = utils.create_dir(save_dir)\n",
    "    if(True):#args.train\n",
    "        model = establish_model(n_features)\n",
    "        model = compile_model(model)\n",
    "        history, model, eval_X, eval_Y = train_model(model)\n",
    "    else:\n",
    "        #loads the most recent saved model\n",
    "        filename = [f for f in os.listdir(save_dir) if \"model\" in f][-1]\n",
    "        model = load_trained_model(os.path.join(save_location, filename))\n",
    "        print(model.summary())\n",
    "        benchmark(model, frames)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
