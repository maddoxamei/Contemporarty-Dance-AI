{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Whether Train or Sample** \n",
    "\n",
    "Don't run in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_dir = \"../data/CSV\"\n",
    "np_data_dir = \"../data/Numpy\"\n",
    "save_dir = \"../logs\"\n",
    "dances = []\n",
    "BATCH_SIZE = 1\n",
    "N_TIMESTEPS = 20\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pull Names of Dance Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames):\n",
    "        filenames[file[0]] = file[1][:-7]\n",
    "    return set(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-Process Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    #filename = os.path.join(csv_data_dir)+filename\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"pos.csv\")\n",
    "    rot_data = pd.read_csv(filename+\"rot.csv\")\n",
    "\n",
    "    #normalization force values from -1 to 1\n",
    "    rot_data = rot_data/180.0\n",
    "\n",
    "    #Add the root (hip) data for spacial movement\n",
    "    rot_data['Hips.pos.x'] = pos_data.pop('Hips.x')\n",
    "    rot_data['Hips.pos.y'] = pos_data.pop('Hips.y')\n",
    "    rot_data['Hips.pos.z'] = pos_data.pop('Hips.z')\n",
    "\n",
    "    #Making movement relative to an origin of 0,0,0 for consistancy within different dances\n",
    "    rot_data['Hips.pos.x'] = rot_data['Hips.pos.x'] + (-1*rot_data['Hips.pos.x'][0])\n",
    "    rot_data['Hips.pos.y'] = rot_data['Hips.pos.y'] + (-1*rot_data['Hips.pos.y'][0])\n",
    "    rot_data['Hips.pos.z'] = rot_data['Hips.pos.z'] + (-1*rot_data['Hips.pos.z'][0])\n",
    "\n",
    "    time = rot_data.pop('time') #maybe change to time change value instead? To indicate speed\n",
    "    data = rot_data.copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Data and Separate Into Samples **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    loadedX = os.path.join(np_data_dir, filename+\"X-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, filename+\"Y-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        print(\"create\")\n",
    "        data = pre_process_data(os.path.join(csv_data_dir, filename))\n",
    "        N_ROWS = data.values.shape[0]\n",
    "        N_COLOMNS = data.values.shape[1]\n",
    "\n",
    "        data = data.iloc[:].values\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "\n",
    "        for i in range(0, N_ROWS - N_TIMESTEPS, 1):\n",
    "            seqIn = data[i: i+N_TIMESTEPS]\n",
    "            seqOut = data[i+N_TIMESTEPS : i+N_TIMESTEPS+1]\n",
    "            dataX.append(seqIn)\n",
    "            dataY.append(seqOut)\n",
    "\n",
    "        #X shape [samples, timesteps, features]\n",
    "        #Y shape [samples, 1, features]\n",
    "        X, Y = np.array(dataX), np.array(dataY)\n",
    "\n",
    "        N_SAMPLES = len(dataX)\n",
    "        Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "        print(\"saving\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set-Up Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(N_COLOMNS, stateful):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', \n",
    "                                input_shape = (N_TIMESTEPS, N_COLOMNS), \n",
    "                                batch_size = BATCH_SIZE, \n",
    "                                return_sequences=True, \n",
    "                                stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = create_model(165, True)\n",
    "    model.compile(optimizer='adam', loss='mse') #metrics=['accuracy']\n",
    "    print(model.summary())\n",
    "    \n",
    "    dances = list(getFileNames())\n",
    "    \n",
    "    for i in range(N_EPOCHS):\n",
    "        print(str(i)+\"/\"+str(N_EPOCHS))\n",
    "        \n",
    "        #define the checkpoint\n",
    "        filepath = os.path.join(save_dir, \"Itt-Weights-Improvement-\"+str(i)+\"-{loss:.4f}.h5\")\n",
    "        best_filepath = os.path.join(save_dir, \"Itt-Weights-Best.h5\")\n",
    "        weight_improvement = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "        best_weights = keras.callbacks.ModelCheckpoint(best_filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [weight_improvement, best_weights]\n",
    "        \n",
    "        for dance in dances:\n",
    "            print(str(i)+\"/\"+str(N_EPOCHS)+\": on dance\", dance)\n",
    "            X, Y = get_sample_data(dance)\n",
    "            #train/fit the model\n",
    "            model.fit(X, Y, callbacks=callbacks_list)\n",
    "        random.shuffle(dances)\n",
    "    \n",
    "    print(\"Done Training, Saving Model\")\n",
    "    savefile = os.path.join(save_dir, \"Itt-Model-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    model.save(savefile) #arch+weight+optimizer state\n",
    "    json_string = model.to_jason() #architecture\n",
    "    \n",
    "    savefile_weights = os.path.join(save_dir, \"Itt-Model-Weights-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    model.save_weights(savefile_weights) #weights\n",
    "    print(\"Done Saving Model\")\n",
    "    \n",
    "    #model = load_model(\"model-Itt-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    #model = model_from_json(json_string)\n",
    "    #model.load_weights(\"modelWeights-Itt-\"+str(N_TIMESTEPS)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sample Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_weights():\n",
    "    minFile = \"\"\n",
    "    minLoss = 100\n",
    "    '''test = {\"weights-improvement-19-1.2765.hdf5\",\n",
    "            \"weights-improvement-20-1.8434.hdf5\",\n",
    "            \"weights-improvement-8-1.1234.hdf5\"}'''\n",
    "    for file in os.listdir(\"./\"):\n",
    "    #for file in test:\n",
    "        if file.endswith(\".hdf5\"):\n",
    "            string = file.split('-')\n",
    "            value = (float)(os.path.splitext(string[len(string)-1])[0])\n",
    "            if(minLoss>value):\n",
    "                minLoss=value\n",
    "                minFile = file\n",
    "    return minFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run Script **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(args.train):\n",
    "    start_time = time.time()\n",
    "    train_model()\n",
    "    print(\"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "else:\n",
    "    print(\"Will Sample in the Future\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
