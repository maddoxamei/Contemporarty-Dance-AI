{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#No module named keras OR cannot import name 'np_utils' if tensorflow.keras\n",
    "#from keras.utils import np_utils\n",
    "#from keras.models import load_model\n",
    "#from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-perameters** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters are based off of the 3 layer model in \n",
    "    Recurrent Neural Networks for Modeling Motion Capture Data \n",
    "    by Mir Khan, Heikki Huttunen, Olli Suominen and Atanas Gotchev\n",
    "\"\"\"\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001) # Maintain a moving (discounted) average of the square of gradients\n",
    "# The folowing initializers are applied to all hidden layers of the model before training \n",
    "weight_initializer = keras.initializers.Orthogonal(gain=1.0, seed=None) # Generates an orthogonal matrix with multiplicative factor equal to the gain\n",
    "recurrent_initializer = tf.keras.initializers.GlorotNormal(seed=None) # Draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (n_input_weight_units + n_output_weight_units))\n",
    "bias_initializer = keras.initializers.Zeros() # Set biases to zero\n",
    "layer_activation = 'tanh'\n",
    "recurrent_activation = 'hard_sigmoid'\n",
    "output_activation = 'linear'\n",
    "\n",
    "batch_size = 32 #number of samples trained before performing one step of gradient update for the loss function (default is stochastic gradient descent)\n",
    "look_back = 50 #how many frames will be used as a history for prediction\n",
    "offset = 1 #how many frames in the future is the prediction going to occur at\n",
    "forecast = 1 #how many frames will be predicted\n",
    "sample_increment = 25 #number of frames between each sample\n",
    "epochs = 30 #maximum number of times all training samples are fed into the model for training\n",
    "units = 1024 #number of nodes in each hidden layer of the network\n",
    "\n",
    "n_features = 165 #number of columns in the input dimension\n",
    "frames = 500 #number of frames the model should generate\n",
    "training_split = .7 #the proportion of the data to use for training\n",
    "validation_split = .2 #the proportion of the data to use for validating during the training phase at the end of each epoch\n",
    "evaluation_split = .1 #(test_split) the proportion of the data for evaluating the model effectiveness after training completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 165 #number of columns in the input dimension\n",
    "frames = 500 #number of frames the model should generate\n",
    "training_split = .7 #the proportion of the data to use for training\n",
    "validation_split = .2 #the proportion of the data to use for validating during the training phase at the end of each epoch\n",
    "evaluation_split = .1 #(test_split) the proportion of the data for evaluating the model effectiveness after training completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_dir = \"/Akamai/MLDance/data/CSV/Raw\" #directory to the csv representation of the dances\n",
    "np_data_dir = \"/Akamai/MLDance/data/Numpy\" #directory to the numpy representation of the dances\n",
    "logs_dir = \"/Akamai/MLDance/logs\" #general output directory\n",
    "\n",
    "data_identifier = \"ts-{}_o-{}_f-{}\".format(look_back, offset,forecast) #data-specific string for use in creating readily identifiable filenames\n",
    "model_identifier = \"units-{}_timesteps-{}\".format(units, look_back) #model-specific string for use in creating readily identifiable filenames\n",
    "save_dir = os.path.join(logs_dir, model_identifier) #output directory for model-specific content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below block in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do NOT run this in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-train]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mmaddox/.local/share/jupyter/runtime/kernel-66fb3281-02d4-45cb-826e-6a7123f47751.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#store_true: default is False, sets the value to True if the respective tag is called\n",
    "#store_false: default is True, sets the value to False if the respective tag is called\n",
    "parser.add_argument('--train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, file=sys.stdout):\n",
    "    \"\"\" Create a visualization of a progress bar updates according to completion status\n",
    "\n",
    "    :param it: job you are trying to create a progress bar for\n",
    "    :type obj (sequence or collection)\n",
    "    :param prefix: The text to display to the left of the status bar\n",
    "    :type str\n",
    "    :param size: total length of the progress bar\n",
    "    :type int\n",
    "    :param file: what to display/write the progress bar to\n",
    "    :type output stream\n",
    "    :return: job you are trying to create a progress bar for\n",
    "    :rtype: obj (sequence or collection)\n",
    "    \"\"\"\n",
    "    count = len(it)\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        file.write(\"%s[%s%s] %i/%i\\r\" % (prefix, \"#\"*x, \".\"*(size-x), j, count))\n",
    "        file.flush()        \n",
    "    show(0)\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    file.write(\"\\n\")\n",
    "    file.flush()\n",
    "    \n",
    "def create_dir(path):\n",
    "    \"\"\" Create the cooresponding directory files for the given path if it does not yet exist\n",
    "\n",
    "    :param path: proposed directory filepath\n",
    "    :type str\n",
    "    :return: created directory filepath\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def getFileNames():\n",
    "    \"\"\" Aggregate the names of unique dances\n",
    "\n",
    "    :return: the dance names where there are csv files for\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames): #enumerating creates an array where 0 corresponds to the index of the file in filenames and 1 corresponds to the filename\n",
    "        filenames[file[0]] = '_'.join(file[1].split(\"_\")[:-1])\n",
    "    return list(set(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pos_data(pos_data):\n",
    "    \"\"\" Alter the position data such that the X-Y plane movement is relative and the Z remains global\n",
    "    \n",
    "    :param pos_data: Hips.X, Hips.Y, and Hips.Z position data\n",
    "    :type pandas.DataFrame\n",
    "    :return: dataframe that contains the altered hip positions\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    vertical_movement = pos_data.pop('Hips.Z')\n",
    "    pos_data = relativize_data(pos_data)\n",
    "    pos_data['Hips.Z'] = vertical_movement\n",
    "    return pos_data\n",
    "    \n",
    "\n",
    "def relativize_data(data):\n",
    "    \"\"\" Alter the position data such that the X-Y plane movement is relative to the previous frame whilst the Z remains global\n",
    "    \n",
    "    :param data: Hips.X and Hips.Y position data for a particular dance\n",
    "    :type pandas.DataFrame\n",
    "    :return: dataframe that contains the hip positions\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    next_frame = data.iloc[len(data)-2] #second to last frame\n",
    "    for index, row in data.iloc[::-1].iterrows(): #itterating through reverse frame order\n",
    "        if(index == 0):\n",
    "            data.iloc[index] = 0\n",
    "        else:\n",
    "            data.iloc[index] = row - data.iloc[index-1]\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\" Separates the data into three different datasets (training, validation, and evaluation) based off the pre-defined split proportions.\n",
    "        Each consecutive dataset is selected from the last samples of the previous one.\n",
    "        \n",
    "        Data is NOT randomly shuffled before spliting to ensure...\n",
    "            chopping the data into windows of consecutive samples is still possible\n",
    "            validation/test results are more realistic, being evaluated on data collected after the model was trained\n",
    "\n",
    "    :param data: the rot or pos data for a particular dance \n",
    "    :type pandas.DataFrame\n",
    "    :return: the spliced datasets cooresponding to training, validation, and evaluation, respectively\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    train_index = int(len(data)*training_split)\n",
    "    validation_index = int(len(data)*(training_split+validation_split))\n",
    "    train_data = data.copy().iloc[0:train_index]\n",
    "    validate_data = data.copy().iloc[train_index:validation_index]\n",
    "    evaluate_data = data.copy().iloc[validation_index:]\n",
    "    return train_data, validate_data, evaluate_data\n",
    "    \n",
    "def standardize_data(data, train_mean, train_std):\n",
    "    \"\"\" Standardizes the dataset values by mean subtraction and division by the standard deviation along each dimension\n",
    "    \n",
    "    :param data: the rot data for a particular dance \n",
    "    :type float\n",
    "    :param train_mean: mean values over for column\n",
    "    :type pandas.DataFrame\n",
    "    :param train_std: standard deviation values for each column\n",
    "    :type float\n",
    "    :return: the standardized dataset\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    #standardize rotation (center the values around zero)\n",
    "    return (data - train_mean) / train_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Process Wrapper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    \"\"\" Alter the position data such that the X-Y plane movement is relative and the Z remains global\n",
    "    \n",
    "    :param filename: Hips.X, Hips.Y, and Hips.Z position data\n",
    "    :type str\n",
    "    :return: dataframe that contains the processed dance frames\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    filename = os.path.join(csv_data_dir, filename)\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"_worldpos.csv\", usecols=['Hips.X','Hips.Y','Hips.Z'])\n",
    "    rot_data = pd.read_csv(filename+\"_rotations.csv\")\n",
    "    data = rot_data.copy()\n",
    "    \n",
    "    # Remove the all the columns were it's all zeroed (End ones)\n",
    "    zeroed_columns = [column for column in data.columns if 'End' in column]\n",
    "    for column in zeroed_columns:\n",
    "        data.pop(column)\n",
    "\n",
    "    # Standardize rotation (force values from -1 to 1)\n",
    "    data = data/180.0\n",
    "    # Standardize rotation (center the values around zero)\n",
    "    '''standarize = lambda x: (x-x.mean()) / x.std()\n",
    "    data = data.pipe(standarize)'''\n",
    "    \n",
    "    \n",
    "    '''standardization_json = \"standardization_{}-{}-{}.json\".format(training_split, validation_split, evaluation_split)\n",
    "    train_mean = 0\n",
    "    train_std = 1\n",
    "    \n",
    "    with open(standardization_json) as file:\n",
    "        standardization = json.load(file)\n",
    "        if filename not in standardization:\n",
    "            train_data = data.iloc[0:int(len(data)*training_split)]\n",
    "            standardization.update({filename:{'mean':train_data.mean().to_dict(),'std':train_data.std().to_dict()}})\n",
    "        train_mean = standardization[filename]['mean']\n",
    "        train_std = standardization[filename]['std']\n",
    "        file.close()          \n",
    "        \n",
    "    #standardize rotation (center the values around zero)\n",
    "    data = standardize_data(data, train_mean, train_std)'''\n",
    "\n",
    "    # Pre-process the position data\n",
    "    pos_data = pre_process_pos_data(pos_data)\n",
    "    \n",
    "        \n",
    "    # Add the root (hip) data for spacial movement\n",
    "    data['Hips.Pos.X'] = pos_data.pop('Hips.X')\n",
    "    data['Hips.Pos.Y'] = pos_data.pop('Hips.Y')\n",
    "    data['Hips.Pos.Z'] = pos_data.pop('Hips.Z')\n",
    "\n",
    "    # Remove the time variable from the dataset\n",
    "    time = data.pop('Time') #maybe change to time change value instead? To indicate speed\n",
    "    return data\n",
    "\n",
    "def post_process_data(rotation_df, position_df):\n",
    "    \"\"\" Un-process the data to transform the values representign the generated dance into something MotionBuidler (visualization program) can interpret.\n",
    "        Un-standardize and un-realativaize the generated dance\n",
    "\n",
    "    :param data: the processed rot and pos data for the generated dance \n",
    "    :type pandas.DataFrame\n",
    "    :return: the unprocessed versions of the rotation and position frames from the generated dance\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    #undo the normalization and standardization of the data\n",
    "    rotation_df = rotation_df*180\n",
    "    position_df['Hips.X'] = position_df['Hips.X'] + hierarchy['offset.x'][0]\n",
    "    position_df['Hips.Y'] = position_df['Hips.Y'] + hierarchy['offset.y'][0]\n",
    "    position_df['Hips.Z'] = position_df['Hips.Z'] + hierarchy['offset.z'][0]\n",
    "    \n",
    "    new_headers = []\n",
    "    joints = [j for j in hierarchy['joint'].to_numpy() if \"End\" not in j]\n",
    "    for j in joints:\n",
    "        new_headers.append(j+\".Z\")\n",
    "        new_headers.append(j+\".X\")\n",
    "        new_headers.append(j+\".Y\")\n",
    "\n",
    "    rotation_df = rotation_df.reindex(columns=new_headers)  \n",
    "    \n",
    "    rotation_df.insert(0, 'time', np.arange(0.0, 0.03333333*len(rotation_df), 0.03333333))\n",
    "    position_df.insert(0, 'time', np.arange(0.0, 0.03333333*len(position_df), 0.03333333))\n",
    "    return rotation_df, position_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save/Load Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(filename):\n",
    "    \"\"\" Fetch the pre-procced data cooresponding to the given dance name\n",
    "\n",
    "    :param filename: the name of the dance to grab the data from\n",
    "    :type str\n",
    "    :return: the pre-processed dance data\n",
    "    :rtype: numpy.Array\n",
    "    \"\"\"\n",
    "    #Establish filenames (X is for input, Y is for expected output)\n",
    "    loaded = os.path.join(np_data_dir, filename)+\"_{}_{}_{}\".format(training_split, validation_split, evaluation_split)\n",
    "    \n",
    "    #If the corresponding numpy file doesn't yet exist, create and save it\n",
    "    if not (os.path.exists(loaded+\".npy\")):\n",
    "        #Print statement for status update\n",
    "        print(\"Creating pre-processed datafile:\", filename)\n",
    "        #load the csv file and establish the number of rows and columns\n",
    "        data = pre_process_data(filename)\n",
    "        #data = data.iloc[:].values #Enables selection/edit of cells in the pandas dataframe\n",
    "        np.save(loaded, data)\n",
    "        print(\"Saved the pre-processed data to\\n\\t\", loaded)\n",
    "\n",
    "    return np.load(loaded+\".npy\")\n",
    "    \n",
    "def save_generated_data(generated_data, original_filename, save_filename):\n",
    "    \"\"\" Save the generated dance to a csv file for bvh converstion later.\n",
    "\n",
    "    :param generated_data: the name of the dance to grab the data from\n",
    "    :type numpy.Array\n",
    "    :param original_filename: dance name the generation seed is from\n",
    "    :type str\n",
    "    :param save_filename: the directory and filename to store the generated dance at\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    rotation = generated_data[:,:162] #get the first 162 columns\n",
    "    position = generated_data[:,[162, 163, 164]] #get the last 3 columns\n",
    "    hierarchy = pd.read_csv(os.path.join(csv_data_dir, \"hierarchy/\"+original_filename.split('_')[0]+\"_hierarchy.csv\"))\n",
    "    \n",
    "    data = pd.read_csv(os.path.join(csv_data_dir, original_file+\"_rotations.csv\"), nrows=0)\n",
    "    c_headers = [c for c in data.columns if 'End' not in c ][1:]\n",
    "    rotation_df = pd.DataFrame(rotation, columns=c_headers)\n",
    "    position_df = pd.DataFrame(position, columns=c_headers[:3])\n",
    "    \n",
    "    rotation_df, position_df = post_process_data(rotation_df, position_df)\n",
    "    \n",
    "    rotation_df.to_csv(save_filename+\"_rot.csv\", index=False)\n",
    "    position_df.to_csv(save_filename+\"_pos.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence the Data (Separate Into Samples) Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_frames(data, dataX, dataY, i):\n",
    "    \"\"\" Create samples (input X and target Y) from the given data\n",
    "\n",
    "    :param data: the data to create samples from\n",
    "    :type numpy.Array\n",
    "    :param dataX: the array to store the input vector data\n",
    "    :type numpy.Array\n",
    "    :param dataY: the array to store the output (target) vectors data\n",
    "    :type numpy.Array\n",
    "    \"\"\"\n",
    "    # Create an input sample cooresponding to the sequence of {look_baack} frame(s) starting at {i}\n",
    "    seqIn = data[i: i+look_back] \n",
    "    # Create an output sample cooresponding to the sequence of {forcast} frame(s) starting at {offset} frame(s) in the future\n",
    "    seqOut = data[i+look_back+offset-1 : i+look_back+offset+forecast-1] \n",
    "    dataX.append(seqIn)\n",
    "    dataY.append(seqOut)\n",
    "    \n",
    "def sequence_data(filename):\n",
    "    \"\"\" Create samples (input X and target Y) from the given data\n",
    "\n",
    "    :param filename: the name of the dance to sequence data from\n",
    "    :type str\n",
    "    :return: the collection of input X and target Y samples\n",
    "    :type tuple\n",
    "    \"\"\"\n",
    "    #load the csv file and establish the number of rows and columns\n",
    "    data = get_processed_data(filename)\n",
    "    N_ROWS = data.shape[0]\n",
    "    N_COLOMNS = data.shape[1]\n",
    "\n",
    "    #data = data.iloc[:].values #Enables selection/edit of cells in the dataset\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "        \n",
    "    #Generate the sequences\n",
    "    for i in range(0, N_ROWS - look_back, sample_increment): #range(start, stop, step) \n",
    "        sequence_frames(data, dataX, dataY, i)\n",
    "\n",
    "    #X shape [samples, timesteps, features]\n",
    "    #Y shape [samples, 1, features]\n",
    "    X, Y = np.array(dataX), np.array(dataY)\n",
    "\n",
    "    N_SAMPLES = len(dataX)\n",
    "    Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    \"\"\" Fetch the pre-sequenced or sampled data from the given dance, or create/save it if it does not yet exist\n",
    "\n",
    "    :param filename: the name of the dance to sample data from\n",
    "    :type str\n",
    "    :return: the collection of input X and target Y samples\n",
    "    :type tuple\n",
    "    \"\"\"    \n",
    "    #Establish filenames (X is for input, Y is for expected output)\n",
    "    loadedX = os.path.join(np_data_dir, filename+\"X_\"+data_identifier)\n",
    "    loadedY = os.path.join(np_data_dir, filename+\"Y_\"+data_identifier)\n",
    "    \n",
    "    #If the corresponding numpy file doesn't yet exist, create and save it\n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        #Print statement for status update\n",
    "        print(\"Creating the sequenced data:\", filename)\n",
    "        X, Y = sequence_data(filename)\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "        print(\"Saved the sequenced data to\\n\\t\", loadedX, \"\\n\\t\", loadedY)\n",
    "\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions related to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-Up Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establish_model(feature_size):\n",
    "    \"\"\" Establish the architecture (layers and how they are connected*) of the model with freshly initialized state for the weights. \n",
    "        There is NO compilation information.\n",
    "\n",
    "    :param feature_size: the number of features in the input/output vector\n",
    "    :type int\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(units, \n",
    "                                input_shape = (look_back, feature_size), \n",
    "                                #batch_size = batch_size, \n",
    "                                activation = layer_activation,\n",
    "                                recurrent_activation = recurrent_activation,\n",
    "                                kernel_initializer = weight_initializer,\n",
    "                                recurrent_initializer = recurrent_initializer,\n",
    "                                bias_initializer = bias_initializer,\n",
    "                                return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(units, \n",
    "                                activation = layer_activation,\n",
    "                                recurrent_activation = recurrent_activation,\n",
    "                                kernel_initializer = weight_initializer,\n",
    "                                recurrent_initializer = recurrent_initializer,\n",
    "                                bias_initializer = bias_initializer,\n",
    "                                return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(units, \n",
    "                                activation = layer_activation,\n",
    "                                recurrent_activation = recurrent_activation,\n",
    "                                kernel_initializer = weight_initializer,\n",
    "                                recurrent_initializer = recurrent_initializer,\n",
    "                                bias_initializer = bias_initializer,\n",
    "                                return_sequences=False))\n",
    "    model.add(keras.layers.Dense(feature_size, activation=output_activation))\n",
    "    return model\n",
    "\n",
    "def compile_model(model):\n",
    "    \"\"\" Compile the given model so that it is ready for training and/or prediction/evaluation\n",
    "\n",
    "    :param model: the model to compile\n",
    "    :type keras.Model\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and Load Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_architecture(model, identifier):\n",
    "    \"\"\" Save the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights and no compilation information from this savefile\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    json_config = model.to_jason()\n",
    "    print(json_donfig)\n",
    "    \n",
    "def save_weights(model, logs=None):\n",
    "    \"\"\" Save the model weights. Ideal for use during training to create checkpoints.\n",
    "        Weights can be loaded into a model (ideally the original checkpointed model) to extract the desired weights/layers into the saved mode\n",
    "\n",
    "    :param model: the model to save the weights from\n",
    "    :type keras.Model\n",
    "    :param logs: dictionary containing current model specs\n",
    "    :type dict\n",
    "    \"\"\"\n",
    "    save_filename = \"weights_{}_\".format(look_back)+model_identifier+\"_loss-{:.2f}_acc-{:.2f}.h5\".format(logs[\"loss\"], logs[\"accuracy\"])\n",
    "    model.save_weights(os.path.join(save_dir, save_filename))\n",
    "    \n",
    "def save_trained_model(model, identifier):\n",
    "    \"\"\" Save the entire model. Model can be loaded and restart training right where you left off\n",
    "        The following are saved:\n",
    "            weight values\n",
    "            Model's architecture\n",
    "            Model's training configuration (what you pass to the .compile() method)\n",
    "            Optimizer and its state, if any (this allows you to restart training)\n",
    "\n",
    "    :param model: the model to save\n",
    "    :type keras.Model\n",
    "    :param identifier: unique string for creating readily identifiable filenames based off model specs\n",
    "    :type str\n",
    "    \"\"\"\n",
    "    model.save(os.path.join(save_dir, \"model-full_\"+identifier+\".h5\"))\n",
    "    \n",
    "def load_architecture(file):\n",
    "    \"\"\" Load the architecture (layers and how they are connected*). \n",
    "        Model can be created with a freshly initialized state for the weights.\n",
    "        There is NO compilation information in this savefile.\n",
    "\n",
    "    :param file: .json file which holds the model's architecture data\n",
    "    :type str\n",
    "    :return: the model's architecture\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return keras.models.model_from_json(file)\n",
    "\n",
    "def load_trained_model(file):\n",
    "    \"\"\" Load the pre-trained model. Compiled when loaded so training/prediction/evaluation can be restarted right where the model left off. \n",
    "\n",
    "    :param file: .h5 file which holds the model's information\n",
    "    :type str\n",
    "    :return: the compiled model\n",
    "    :type keras.Model\n",
    "    \"\"\"\n",
    "    return keras.models.load_model(file, compile=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \"\"\" A class to create custom callback options. This overrides a set of methods called at various stages of training, testing, and predicting. \n",
    "        Callbacks are useful to get a view on internal states and statistics of the model during training.\n",
    "            Callback list can be passed for .fit(), .evaluate(), and .predict() methods\n",
    "            \n",
    "        NOT CURRENTLY IN USE\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\" Trains the model with the dance data.\n",
    "        The History object's History.history attribute is a record of training loss values and metrics values at successive epochs, \n",
    "            as well as cooresponding validation values (if applicable).  \n",
    "\n",
    "    :param model: the model to train\n",
    "    :type keras.Model\n",
    "    :return: the class containing the training metric information and the trained model\n",
    "    :type tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    dances = getFileNames()\n",
    "    callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)]#CustomCallback()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for dance in dances:\n",
    "            print(\"Epoch \",str(i)+\"/\"+str(epochs)+\":\", dance)\n",
    "            X, Y = get_sample_data(dance)\n",
    "            \n",
    "            print (\"X Shape:\", X.shape)\n",
    "            print (\"Y Shape:\", Y.shape)\n",
    "            #train/fit the model\n",
    "            history = model.fit(X, Y, \n",
    "                      batch_size = batch_size, \n",
    "                      callbacks=callbacks_list, \n",
    "                      shuffle=False, \n",
    "                      validation_split=0.2, \n",
    "                      epochs=1, \n",
    "                      verbose=1)\n",
    "        save_weights(model, {'loss':keras.metrics.MeanSquaredError().result(), 'accuracy':keras.metrics.Accuracy().result()})\n",
    "        if(i%3==0):\n",
    "            save_trained_model(model_identifier+\"epoch-{}.h5\".format(i))\n",
    "        random.shuffle(dances)\n",
    "    \n",
    "    print(\"Done Training\")\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample/Run Model (Make Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, n_frames, random=False):\n",
    "    \"\"\" Generate a dance sequence with the given model\n",
    "\n",
    "    :param model: the model to use for prediction \n",
    "    :type keras.Model\n",
    "    :param n_frames: the number of frames the model should generate\n",
    "    :type int\n",
    "    \"\"\"\n",
    "    #select random dance for seed\n",
    "    dances = getFileNames()\n",
    "    seed_dance_index = random.randint(0, len(dances) - 1)\n",
    "    dance = get_processed_data(dances[seed_dance_index])\n",
    "    seed = dance[:N_TIMESTEPS]\n",
    "    if random:\n",
    "        #select random frame(s) for seed\n",
    "        seed_frame_index = random.randint(0, len(dance) - (look_back+1))\n",
    "        seed = dance[seed_frame_index:seed_frame_index+look_back]\n",
    "    \n",
    "    print(\"Generating dance with seed from\", dances[seed_dance_index])\n",
    "    #for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    for diversity in [1.0]:\n",
    "        start_time = time.time()\n",
    "        generated = seed\n",
    "        for i in progressbar(range(n_frames),\"{} Progress: \".format(diversity)):\n",
    "            preds = model.predict(np.array([generated[-look_back:]]), verbose=0)[0]\n",
    "            generated = np.vstack((generated, preds))\n",
    "        filename = os.path.join(save_dir, \"generated_dance_{}-frames_{}-diversity\".format(n_frames, diversity))\n",
    "        save_generated_data(generated, dances[seed_dance_index], filename)\n",
    "        print(\"\\tTotal Elapsed time (in sec.):\", time.time()-start_time)\n",
    "        print(\"\\tSaved to\", save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 50, 1024)          4874240   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 165)               169125    \n",
      "=================================================================\n",
      "Total params: 21,828,773\n",
      "Trainable params: 21,828,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch  0/30: Theodora_Happy_1\n",
      "X Shape: (28, 50, 165)\n",
      "Y Shape: (28, 165)\n",
      "Train on 22 samples, validate on 6 samples\n",
      "22/22 [==============================] - 7s 297ms/sample - loss: 24.7703 - accuracy: 0.0000e+00 - val_loss: 25.4479 - val_accuracy: 0.1667\n",
      "Epoch  0/30: Theodora_Mix_2\n",
      "Creating the sequenced data: Theodora_Mix_2\n",
      "Creating pre-processed datafile: Theodora_Mix_2\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Mix_2_0.7_0.2_0.1\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Mix_2X_ts-50_o-1_f-1 \n",
      "\t /Akamai/MLDance/data/Numpy/Theodora_Mix_2Y_ts-50_o-1_f-1\n",
      "X Shape: (143, 50, 165)\n",
      "Y Shape: (143, 165)\n",
      "Train on 114 samples, validate on 29 samples\n",
      "114/114 [==============================] - 7s 65ms/sample - loss: 17.6583 - accuracy: 0.3421 - val_loss: 22.7938 - val_accuracy: 0.3448\n",
      "Epoch  0/30: Olivia_Sad_0\n",
      "Creating the sequenced data: Olivia_Sad_0\n",
      "Creating pre-processed datafile: Olivia_Sad_0\n",
      "Saved the pre-processed data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Sad_0_0.7_0.2_0.1\n",
      "Saved the sequenced data to\n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Sad_0X_ts-50_o-1_f-1 \n",
      "\t /Akamai/MLDance/data/Numpy/Olivia_Sad_0Y_ts-50_o-1_f-1\n",
      "X Shape: (28, 50, 165)\n",
      "Y Shape: (28, 165)\n",
      "Train on 22 samples, validate on 6 samples\n",
      "22/22 [==============================] - 2s 82ms/sample - loss: 19.2131 - accuracy: 0.4091 - val_loss: 1.1879 - val_accuracy: 0.0000e+00\n",
      "Epoch  0/30: Vasso_Mix_2\n",
      "Creating the sequenced data: Vasso_Mix_2\n",
      "Creating pre-processed datafile: Vasso_Mix_2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4ddbecb2006a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-4ddbecb2006a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestablish_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s hours ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-db751ade320e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"X Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-36c234fa6dbb>\u001b[0m in \u001b[0;36mget_sample_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#Print statement for status update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating the sequenced data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadedX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadedY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ab92903957f5>\u001b[0m in \u001b[0;36msequence_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#load the csv file and establish the number of rows and columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_processed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mN_ROWS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mN_COLOMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8a19c9fb4a1b>\u001b[0m in \u001b[0;36mget_processed_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating pre-processed datafile:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#load the csv file and establish the number of rows and columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#data = data.iloc[:].values #Enables selection/edit of cells in the pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3a1ebfabd32a>\u001b[0m in \u001b[0;36mpre_process_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpos_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_worldpos.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hips.X'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Hips.Y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Hips.Z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_rotations.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\" Driver function to control what is run and when if this is the main python script being ran.\n",
    "        As the project was developed in a jupyter notebook, everything is self-contained in the main file.\n",
    "        Any expansion, however, would be able to use the predefined classes and functions for whatever purpose without running anything.\n",
    "    \"\"\"\n",
    "    save_location = create_dir(save_dir)\n",
    "    if(args.train):\n",
    "        start_time = time.time()\n",
    "        model = establish_model(n_features)\n",
    "        train_model(compile_model(model))\n",
    "        print(\"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "    else:\n",
    "        #loads the most recent saved model\n",
    "        filename = [f for f in os.listdir(save_dir) if \"model\" in f][-1]\n",
    "        model = load_trained_model(os.path.join(save_location, filename))\n",
    "        print(model.summary())\n",
    "        benchmark(frames, model)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
