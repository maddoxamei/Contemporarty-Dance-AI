{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random, argparse, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine Whether Train or Sample** \n",
    "\n",
    "Don't run in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-train]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1034/jupyter/kernel-b07a2a1f-3411-46b4-a7ea-d7e848fd0611.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-train', action=\"store_true\",\n",
    "                   help='True: Train on dataset, False: Sample with trained model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_dir = \"../data/CSV\"\n",
    "np_data_dir = \"../data/Numpy\"\n",
    "save_dir = \"../logs\"\n",
    "dances = []\n",
    "BATCH_SIZE = 1\n",
    "N_TIMESTEPS = 20\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pull Names of Dance Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames):\n",
    "        filenames[file[0]] = file[1][:-7]\n",
    "    return set(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-Process Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    #filename = os.path.join(csv_data_dir)+filename\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"pos.csv\")\n",
    "    rot_data = pd.read_csv(filename+\"rot.csv\")\n",
    "\n",
    "    #normalization force values from -1 to 1\n",
    "    rot_data = rot_data/180.0\n",
    "\n",
    "    #Add the root (hip) data for spacial movement\n",
    "    rot_data['Hips.pos.x'] = pos_data.pop('Hips.x')\n",
    "    rot_data['Hips.pos.y'] = pos_data.pop('Hips.y')\n",
    "    rot_data['Hips.pos.z'] = pos_data.pop('Hips.z')\n",
    "\n",
    "    #Making movement relative to an origin of 0,0,0 for consistancy within different dances\n",
    "    rot_data['Hips.pos.x'] = rot_data['Hips.pos.x'] + (-1*rot_data['Hips.pos.x'][0])\n",
    "    rot_data['Hips.pos.y'] = rot_data['Hips.pos.y'] + (-1*rot_data['Hips.pos.y'][0])\n",
    "    rot_data['Hips.pos.z'] = rot_data['Hips.pos.z'] + (-1*rot_data['Hips.pos.z'][0])\n",
    "\n",
    "    time = rot_data.pop('time') #maybe change to time change value instead? To indicate speed\n",
    "    data = rot_data.copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Data and Separate Into Samples **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    loadedX = os.path.join(np_data_dir, filename+\"X-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, filename+\"Y-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        print(\"create\")\n",
    "        data = pre_process_data(os.path.join(csv_data_dir, filename))\n",
    "        N_ROWS = data.values.shape[0]\n",
    "        N_COLOMNS = data.values.shape[1]\n",
    "\n",
    "        data = data.iloc[:].values\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "\n",
    "        for i in range(0, N_ROWS - N_TIMESTEPS, 1):\n",
    "            seqIn = data[i: i+N_TIMESTEPS]\n",
    "            seqOut = data[i+N_TIMESTEPS : i+N_TIMESTEPS+1]\n",
    "            dataX.append(seqIn)\n",
    "            dataY.append(seqOut)\n",
    "\n",
    "        #X shape [samples, timesteps, features]\n",
    "        #Y shape [samples, 1, features]\n",
    "        X, Y = np.array(dataX), np.array(dataY)\n",
    "\n",
    "        N_SAMPLES = len(dataX)\n",
    "        Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "        print(\"saving\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data():\n",
    "    loadedX = os.path.join(np_data_dir, \"CombinedX-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, \"CombinedY-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        dances = list(getFileNames())\n",
    "        X, Y = get_sample_data(dances[0])\n",
    "\n",
    "        for dance in dances:\n",
    "            if not dance == dances[0]:\n",
    "                dataX, dataY = get_sample_data(dance)\n",
    "                print(\"Combining dance:\", dance)\n",
    "                X = np.concatenate((X, dataX), axis = 0)\n",
    "                Y = np.concatenate((Y, dataY), axis = 0)\n",
    "        print(\"Finished Combining, X Shape: \", X.shape, \"Y Shape: \", Y.shape)\n",
    "        print(\"saving\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "    return np.load(loadedX+\".npy\"), np.load(loadedY+\".npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set-Up Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(N_COLOMNS, stateful):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', \n",
    "                                input_shape = (N_TIMESTEPS, N_COLOMNS), \n",
    "                                batch_size = BATCH_SIZE, \n",
    "                                return_sequences=True, \n",
    "                                stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = create_model(165, True)\n",
    "    model.compile(optimizer='adam', loss='mse') #metrics=['accuracy']\n",
    "    print(model.summary())\n",
    "    \n",
    "    #define the checkpoint\n",
    "    filepath = os.path.join(save_dir, \"Com-Weights-Improvement-{epoch:02d}-{loss:.4f}.hdf5\")\n",
    "    best_filepath = os.path.join(save_dir, \"Com-Weights-Best.hdf5\")\n",
    "    weight_improvement = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "    best_weights = keras.callbacks.ModelCheckpoint(best_filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [weight_improvement, best_weights]\n",
    "    \n",
    "    X, Y = combine_data()\n",
    "    \n",
    "    model.fit(X, Y, epochs = N_EPOCHS, callbacks=callbacks_list)\n",
    "    savefile = os.path.join(save_dir, \"Com-Model-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    savefile_weights = os.path.join(save_dir, \"Com-Model-Weights-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    model.save(savefile) #arch+weight+optimizer state\n",
    "    json_string = model.to_jason() #architecture\n",
    "    model.save_weights(savefile_weights) #weights\n",
    "    \n",
    "    #model = load_model(\"model-Itt-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    #model = model_from_json(json_string)\n",
    "    #model.load_weights(\"modelWeights-Itt-\"+str(N_TIMESTEPS)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1002eeff2ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s hours ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "if(args.train):\n",
    "    start_time = time.time()\n",
    "    #train_model()\n",
    "    print(\"--- %s hours ---\" % ((time.time() - start_time)/3600))\n",
    "else:\n",
    "    print(\"Will Sample in the Future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
