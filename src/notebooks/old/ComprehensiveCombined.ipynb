{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependancies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_dir = \"../data/CSV\"\n",
    "np_data_dir = \"../data/Numpy\"\n",
    "save_dir = \"../logs\"\n",
    "dances = []\n",
    "BATCH_SIZE = 1\n",
    "N_TIMESTEPS = 20\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pull Names of Dance Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames():\n",
    "    filenames = [f for f in os.listdir(csv_data_dir) if f.endswith('.csv')]\n",
    "    for file in enumerate(filenames):\n",
    "        filenames[file[0]] = file[1][:-7]\n",
    "    return set(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Pre-Process Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filename):\n",
    "    #filename = os.path.join(data_dir)+filename\n",
    "    \n",
    "    pos_data = pd.read_csv(filename+\"pos.csv\")\n",
    "    rot_data = pd.read_csv(filename+\"rot.csv\")\n",
    "\n",
    "    #normalization force values from -1 to 1\n",
    "    rot_data = rot_data/180.0\n",
    "\n",
    "    #Add the root (hip) data for spacial movement\n",
    "    rot_data['Hips.pos.x'] = pos_data.pop('Hips.x')\n",
    "    rot_data['Hips.pos.y'] = pos_data.pop('Hips.y')\n",
    "    rot_data['Hips.pos.z'] = pos_data.pop('Hips.z')\n",
    "\n",
    "    #Making movement relative to an origin of 0,0,0 for consistancy within different dances\n",
    "    rot_data['Hips.pos.x'] = rot_data['Hips.pos.x'] + (-1*rot_data['Hips.pos.x'][0])\n",
    "    rot_data['Hips.pos.y'] = rot_data['Hips.pos.y'] + (-1*rot_data['Hips.pos.y'][0])\n",
    "    rot_data['Hips.pos.z'] = rot_data['Hips.pos.z'] + (-1*rot_data['Hips.pos.z'][0])\n",
    "\n",
    "    time = rot_data.pop('time') #maybe change to time change value instead? To indicate speed\n",
    "    data = rot_data.copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Data and Separate Into Samples **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(filename):\n",
    "    data = pre_process_data(os.path.join(csv_data_dir, filename))\n",
    "    N_ROWS = data.values.shape[0]\n",
    "    N_COLOMNS = data.values.shape[1]\n",
    "    \n",
    "    data = data.iloc[:].values\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "\n",
    "    for i in range(0, N_ROWS - N_TIMESTEPS, 1):\n",
    "        seqIn = data[i: i+N_TIMESTEPS]\n",
    "        seqOut = data[i+N_TIMESTEPS : i+N_TIMESTEPS+1]\n",
    "        dataX.append(seqIn)\n",
    "        dataY.append(seqOut)\n",
    "\n",
    "    return dataX, dataY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set-Up Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(N_COLOMNS, stateful):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', \n",
    "                                input_shape = (N_TIMESTEPS, N_COLOMNS), \n",
    "                                batch_size = BATCH_SIZE, \n",
    "                                return_sequences=True, \n",
    "                                stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(256, activation='relu', stateful=stateful))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(N_COLOMNS, activation='tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f1f6088914b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_TIMESTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f1f6088914b6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m165\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#metrics=['accuracy']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = create_model(165, True)\n",
    "    model.compile(optimizer='adam', loss='mse') #metrics=['accuracy']\n",
    "    print(model.summary())\n",
    "    \n",
    "    #define the checkpoint\n",
    "    filepath = os.path.join(save_dir, \"com-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\")\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    loadedX = os.path.join(np_data_dir, \"Combined-X-\"+str(N_TIMESTEPS))\n",
    "    loadedY = os.path.join(np_data_dir, \"Combined-Y-\"+str(N_TIMESTEPS))\n",
    "    \n",
    "    if not (os.path.exists(loadedX+\".npy\") and os.path.exists(loadedY+\".npy\")):\n",
    "        print(\"create\")\n",
    "    \n",
    "        dances = list(getFileNames())   \n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for dance in dances:\n",
    "            x, y = get_sample_data(dance)\n",
    "            print(dance)\n",
    "            X.append(x)\n",
    "            Y.append(Y)\n",
    "\n",
    "        #X shape [samples, timesteps, features]\n",
    "        #Y shape [samples, 1, features]\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "\n",
    "        N_SAMPLES = len(X)\n",
    "        Y = np.reshape(Y, (N_SAMPLES, N_COLOMNS))\n",
    "        \n",
    "        print(\"saving\")\n",
    "        np.save(loadedX, X)\n",
    "        np.save(loadedY, Y)\n",
    "    \n",
    "    \n",
    "    X = np.load(loadedX+\".npy\")\n",
    "    Y = np.load(loadedY+\".npy\")\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    #train/fit the model\n",
    "\n",
    "    model.fit(X, Y, epochs = N_EPOCHS, callbacks=callbacks_list)\n",
    "    model.save(\"model-Com-\"+str(N_TIMESTEPS)+\".h5\") #arch+weight+optimizer state\n",
    "    #json_string = model.to_jason() #architecture\n",
    "    #model.save_weights(\"modelWeights-Com-\"+str(N_TIMESTEPS)+\".h5\") #weights\n",
    "    \n",
    "    #model = load_model(\"model-Com-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "    #model = model_from_json(json_string)\n",
    "    #model.load_weights(\"modelWeights-Com-\"+str(N_TIMESTEPS)+\".h5\")\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sample Model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_weights():\n",
    "    minFile = \"\"\n",
    "    minLoss = 100\n",
    "    '''test = {\"weights-improvement-19-1.2765.hdf5\",\n",
    "            \"weights-improvement-20-1.8434.hdf5\",\n",
    "            \"weights-improvement-8-1.1234.hdf5\"}'''\n",
    "    for file in os.listdir(\"./\"):\n",
    "    #for file in test:\n",
    "        if file.endswith(\".hdf5\"):\n",
    "            string = file.split('-')\n",
    "            value = (float)(os.path.splitext(string[len(string)-1])[0])\n",
    "            if(minLoss>value):\n",
    "                minLoss=value\n",
    "                minFile = file\n",
    "    return minFile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
